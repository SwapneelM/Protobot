RedirectNeural networkNeural network disambiguation
Machine learning bar
Use dmy datesdateJune 2013
FileColored neural networksvgthumb300pxAn artificial neural network is an interconnected group of nodes akin to the vast network of neurons in a brain Here each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another
Artificial neural networks ANNs or Connectionismconnectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brainsrefCite weburlhttpswwwfrontiersinorgresearchtopics4817artificialneuralnetworksasmodelsofneuralinformationprocessingtitleArtificial Neural Networks as Models of Neural Information Processing  Frontiers Research Topiclanguageenaccessdate20180220ref Such systems learn ie progressively improve performance on tasks by considering examples generally without taskspecific programming For example in image recognition they might learn to identify images that contain cats by analyzing example images that have been manually Labeled datalabeled as cat or no cat and using the results to identify cats in other images They do this without any a priori knowledgea priori knowledge about cats eg that they have fur tails whiskers and catlike faces Instead they evolve their own set of relevant characteristics from the learning material that they process

An ANN is based on a collection of connected units or nodes called artificial neurons a simplified version of biological neurons in an animal brain Each connection a simplified version of a synapse between artificial neurons can transmit a signal from one to another The artificial neuron that receives the signal can process it and then signal artificial neurons connected to it

In common ANN implementations the signal at a connection between artificial neurons is a real number and the output of each artificial neuron is calculated by a nonlinear function of the sum of its inputs Artificial neurons and connections typically have a weight mathematicsweight that adjusts as learning proceeds The weight increases or decreases the strength of the signal at a connection Artificial neurons may have a threshold such that only if the aggregate signal crosses that threshold is the signal sent Typically artificial neurons are organized in layers Different layers may perform different kinds of transformations on their inputs Signals travel from the first input to the last output layer possibly after traversing the layers multiple times

The original goal of the ANN approach was to solve problems in the same way that a human brain would However over time attention focused on matching specific tasks leading to deviations from biology ANNs have been used on a variety of tasks including computer vision speech recognition machine translation social network filtering playing Board gameboard and video games and medical diagnosis
toclimit3

History
Warren McCulloch and Walter Pittsrefcite journallastMcCullochfirstWarrenauthor2Walter PittstitleA Logical Calculus of Ideas Immanent in Nervous ActivityjournalBulletin of Mathematical Biophysicsyear1943volume5pages115133doi101007BF02478259issue4ref 1943 created a computational model for neural networks based on mathematics and algorithms called threshold logic This model paved the way for neural network research to split into two approaches One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence This work led to work on nerve networks and their link to Finite state machinefinite automatarefCite newsurlhttpswwwdegruytercomviewbooks978140088261897814008826180029781400882618002xmltitleRepresentation of Events in Nerve Nets and Finite AutomatalastKleenefirstSCdateworkAnnals of Mathematics Studiesaccessdate20170617archiveurlarchivedatedeadurlpublisherPrinceton University Pressyear1956issue34pages341languageenref

Hebbian learning

In the late 1940s Donald O HebbDO Hebbrefcite bookurlgoogle books plainurly idddB4AgAAQBAJtitleThe Organization of BehaviorlastHebbfirstDonaldpublisherWileyyear1949isbn9781135631901locationNew Yorkpagesref created a learning hypothesis based on the mechanism of Neuroplasticityneural plasticity that became known as Hebbian learning Hebbian learning is unsupervised learning This evolved into models for long term potentiation Researchers started applying these ideas to computational models in 1948 with unorganized machineTurings Btype machines

Farley and Wesley A ClarkClarkrefcite journallastFarleyfirstBGauthor2WA ClarktitleSimulation of SelfOrganizing Systems by Digital ComputerjournalIRE Transactions on Information Theoryyear1954volume4pages7684doi101109TIT19541057468issue4ref 1954 first used computational machines then called calculators to simulate a Hebbian network Other neural network computational machines were created by Nathaniel Rochester computer scientistRochester Holland Habit and Duda 1956refcite journallastRochesterfirstN author2JH Holland author3LH Habit author4WL DudatitleTests on a cell assembly theory of the action of the brain using a large digital computerjournalIRE Transactions on Information Theoryyear1956volume2pages8093doi101109TIT19561056810issue3ref

Frank RosenblattRosenblattrefcite journallastRosenblattfirstFtitleThe Perceptron A Probabilistic Model For Information Storage And Organization In The BrainjournalPsychological Reviewyear1958volume65pages386408doi101037h0042519pmid13602029issue6citeseerx10115883775ref 1958 created the perceptron an algorithm for pattern recognition With mathematical notation Rosenblatt described circuitry not in the basic perceptron such as the exclusiveor circuit that could not be processed by neural networks at the timeref nameWerbos 1975cite bookurlgoogle books plainurly idz81XmgEACAAJtitleBeyond Regression New Tools for Prediction and Analysis in the Behavioral ScienceslastWerbosfirstPJpublisheryear1975isbnlocationpagesref

In 1959 a biological model proposed by Nobel laureates David H HubelHubel and Torsten WieselWiesel was based on their discovery of two types of cells in the primary visual cortex simple cells and complex cellsrefcite bookurlhttpsbooksgooglecombooksid8YrxWojxUA4CpgPA106titleBrain and visual perception the story of a 25year collaborationpublisherOxford University Press USyear2005isbn9780195176186page106authorDavid H Hubel and Torsten N Wieselref

The first functional networks with many layers were published by Alexey Grigorevich IvakhnenkoIvakhnenko and Lapa in 1965 becoming the Group method of data handlingGroup Method of Data Handlingref nameSCHIDHUB2cite journallastSchmidhuberfirstJyear2015titleDeep Learning in Neural Networks An OverviewjournalNeural Networksvolume61pages85117arxiv14047828doi101016jneunet201409003pmid25462637refref nameivak1965cite bookurlgoogle books plainurly idFhwVNQAACAAJtitleCybernetic Predicting DeviceslastIvakhnenkofirstA GpublisherCCM Information Corporationyear1973refref nameivak1967cite bookurlgoogle books plainurly idrGFgAAAAMAAJtitleCybernetics and forecasting techniqueslast2Grigor πevich Lapafirst2ValentinpublisherAmerican Elsevier Pub Coyear1967first1A Glast1Ivakhnenkoref

Neural network research stagnated after machine learning research by Marvin MinskyMinsky and Seymour PapertPapert 1969refcite bookurlgoogle books plainurly idOw1OAQAAIAAJtitlePerceptrons An Introduction to Computational GeometrylastMinskyfirstMarvinfirst2SeymourpublisherMIT Pressyear1969isbn0262630222locationpagesauthor2Papertref who discovered two key issues with the computational machines that processed neural networks The first was that basic perceptrons were incapable of processing the exclusiveor circuit The second was that computers didnt have enough processing power to effectively handle the work required by large neural networks Neural network research slowed until computers achieved far greater processing power

Much of artificial intelligence had focused on highlevel symbolic models that are processed by using algorithms characterized for example by expert systems with knowledge embodied in ifthen rules until in the late 1980s research expanded to lowlevel subsymbolic machine learning characterized by knowledge embodied in the parameters of a cognitive modelCitation neededdateAugust 2017reasonReliable source needed for the whole sentence

 Backpropagation 
A key trigger for renewed interest in neural networks and learning was Paul WerbosWerboss 1975 backpropagation algorithm that effectively solved the exclusiveor problem and more generally accelerated the training of multilayer networks Backpropagation distributed the error term back up through the layers by modifying the weights at each noderef nameWerbos 1975 

In the mid1980s parallel distributed processing became popular under the name connectionism David E RumelhartRumelhart and James McClelland psychologistMcClelland 1986 described the use of connectionism to simulate neural processesrefcite bookurlgoogle books plainurly iddavmLgzusB8CtitleParallel Distributed Processing Explorations in the Microstructure of CognitionlastRumelhartfirstDEfirst2JamespublisherMIT Pressyear1986isbn9780262631105locationCambridgepagesauthor2McClellandref

Support vector machines and other much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity

Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pretraining while available computing power increased through the use of GPUs and distributed computing Neural networks were deployed on a large scale particularly in image and visual recognition problems This became known as deep learning

In 1992 Convolutional neural networkPooling layermaxpooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognitionref nameWeng1992J Weng N Ahuja and T S Huang httpwwwcsemsueduwengresearchCresceptronIJCNN1992pdf Cresceptron a selforganizing neural network which grows adaptively Proc International Joint Conference on Neural Networks Baltimore Maryland vol I pp 576581 June 1992refref nameWeng19932J Weng N Ahuja and T S Huang httpwwwcsemsueduwengresearchCresceptronICCV1993pdf Learning recognition and segmentation of 3D objects from 2D images Proc 4th International Conf Computer Vision Berlin Germany pp 121128 May 1993refref nameWeng1997J Weng N Ahuja and T S Huang httpwwwcsemsueduwengresearchCresceptronIJCVpdf Learning recognition and segmentation using the Cresceptron International Journal of Computer Vision vol 25 no 2 pp 105139 Nov 1997ref
In 2010 Backpropagation training through Convolutional neural networkPooling layermaxpooling was accelerated by GPUs and shown to perform better than other pooling variantsref nameScherer2010Dominik Scherer Andreas C M√ºller and Sven Behnke httpswwwaisunibonndepapersicann2010maxpoolpdf Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition In 20th International Conference Artificial Neural Networks ICANN pp 92101 2010 httpsdoiorg101007978364215825410ref

The vanishing gradient problem affects manylayered Feedforward neural networkfeedforward networks that used backpropagation and also recurrent neural networks RNNsref nameHOCH19912S Hochreiter httppeopleidsiachjuergenSeppHochreiter1991ThesisAdvisorSchmidhuberpdf Untersuchungen zu dynamischen neuronalen Netzen Diploma thesis Institut f Informatik Technische Univ Munich Advisor J Schmidhuber 1991refref nameHOCH2001cite bookurlgoogle books plainurly idNWOcMVA64aACtitleA Field Guide to Dynamical Recurrent NetworkslastHochreiterfirstSlast2et aldate15 January 2001publisherJohn Wiley  Sonsyearisbn9780780353695locationpageschapterGradient flow in recurrent nets the difficulty of learning longterm dependencieseditorlast2Kremereditorfirst2Stefan Ceditorfirst1John Feditorlast1Kolenref As errors propagate from layer to layer they shrink exponentially with the number of layers impeding the tuning of neuron weights that is based on those errors particularly affecting deep networks

To overcome this problem J√ºrgen SchmidhuberSchmidhuber adopted a multilevel hierarchy of networks 1992 pretrained one level at a time by unsupervised learning and finetuned by backpropagationref nameSCHMID1992J Schmidhuber Learning complex extended sequences using the principle of history compression Neural Computation 4 pp 234242 1992ref Behnke 2003 relied only on the sign of the gradient Rproprefcite bookurlhttpwwwaisunibonndebooksLNCS2766pdftitleHierarchical Neural Networks for Image InterpretationpublisherSpringeryear2003seriesLecture Notes in Computer Sciencevolume2766authorSven Behnkeref on problems such as image reconstruction and face localization

Geoffrey HintonHinton et al 2006 proposed learning a highlevel representation using successive layers of binary or realvalued latent variables with a restricted Boltzmann machineref namesmolensky1986cite bookurlhttpportalacmorgcitationcfmid104290titleParallel Distributed Processing Explorations in the Microstructure of Cognitionyear1986editorsD E Rumelhart J L McClelland  the PDP Research Groupvolume1pages194281chapterInformation processing in dynamical systems Foundations of harmony theorylast1Smolenskyfirst1Pauthorlink1Paul Smolenskyref to model each layer Once sufficiently many layers have been learned the deep architecture may be used as a generative model by reproducing the data when sampling down the model an ancestral pass from the top level feature activationsref namehinton2006cite journallast2Osinderofirst2Slast3Tehfirst3Yyear2006titleA fast learning algorithm for deep belief netsurlhttpwwwcstorontoeduhintonabspsfastncpdfjournalNeural Computation journalNeural Computationvolume18issue7pages15271554doi101162neco20061871527pmid16764513last1Hintonfirst1G Eauthorlink1Geoffrey HintonrefrefCite journalyear2009titleDeep belief networksurlhttpwwwscholarpediaorgarticleDeepbeliefnetworksjournalScholarpediavolume4issue5pages5947doi104249scholarpedia5947pmcpmidlast1Hintonfirst1Gbibcode2009SchpJ45947Href In 2012 Andrew NgNg and Jeff Dean computer scientistDean created a network that learned to recognize higherlevel concepts such as cats only from watching unlabeled images taken from YouTube videosref nameng2012cite arXiveprint11126209first2Jefflast2DeantitleBuilding Highlevel Features Using Large Scale Unsupervised Learninglast1Ngfirst1Andrewyear2012classcsLGref

Hardwarebased designs
Computational devices were created in CMOS for both biophysical simulation and neuromorphic computing Nanodevicesrefcite journal  last1  Yang  first1  J J  last2  Pickett  first2  M D  last3  Li  first3  X M  last4  Ohlberg  first4  D A A  last5  Stewart  first5  D R  last6  Williams  first6  R S  year  2008  title   Memristive switching mechanism for metaloxidemetal nanodevices url   journal  Nat Nanotechnol  volume  3  issue  7 pages  429433  doi  101038nnano2008160 ref for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally Analog signalanalog rather than Digital datadigital even though the first implementations may use digital devicesrefcite journal  last1  Strukov  first1  D B  last2  Snider  first2  G S  last3  Stewart  first3  D R  last4  Williams  first4  R S  year  2008  title   The missing memristor found url   journal  Nature  volume  453  issue  7191 pages  8083  doi101038nature06932  pmid18451858 bibcode  2008Natur45380S ref Ciresan and colleagues 2010ref name3Cite journallastCire≈üanfirstDan Claudiulast2Meierfirst2Uelilast3Gambardellafirst3Luca Marialast4Schmidhuberfirst4J√ºrgendate20100921titleDeep Big Simple Neural Nets for Handwritten Digit Recognitionurlhttpwwwmitpressjournalsorgdoi101162NECOa00052journalNeural Computationvolume22issue12pages32073220doi101162necoa00052issn08997667ref in Schmidhubers group showed that despite the vanishing gradient problem GPUs makes backpropagation feasible for manylayered feedforward neural networks

 Contests 
Between 2009 and 2012 recurrent neural networks and deep feedforward neural networks developed in J√ºrgen SchmidhuberSchmidhubers research group won eight international competitions in pattern recognition and machine learningrefhttpwwwkurzweilainethowbioinspireddeeplearningkeepswinningcompetitions 2012 Kurzweil AI Interview with J√ºrgen Schmidhuber on the eight competitions won by his Deep Learning team 20092012refrefCite weburlhttpwwwkurzweilainethowbioinspireddeeplearningkeepswinningcompetitionstitleHow bioinspired deep learning keeps winning competitions  KurzweilAIlastfirstdatewebsitewwwkurzweilainetlanguageenUSarchiveurlarchivedatedeadurlaccessdate20170616ref For example the bidirectional and multidimensional long shortterm memory LSTMrefGraves Alex and Schmidhuber J√ºrgen httpwwwidsiachjuergennips2009pdf Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks in Bengio Yoshua Schuurmans Dale Lafferty John Williams Chris K I and Culotta Aron eds Advances in Neural Information Processing Systems 22 NIPS22 710 December 2009 Vancouver BC Neural Information Processing Systems NIPS Foundation 2009 pp 545552
refref namegraves 855 ref namegraves20093Cite journallast2Schmidhuberfirst2J√ºrgendate2009editorlastBengioeditorfirstYoshuatitleOffline Handwriting Recognition with Multidimensional Recurrent Neural Networksurlhttpspapersnipsccpaper3449offlinehandwritingrecognitionwithmultidimensionalrecurrentneuralnetworksjournalNeural Information Processing Systems NIPS Foundationvolumepages545552viaeditorlast2Schuurmanseditorfirst2Daleeditorlast3Laffertyeditorfirst3Johneditorlast4Williamseditorfirst4Chris editorK Ieditorlast5Culottaeditorfirst5Aronlast1Gravesfirst1AlexrefrefCite journallastGravesfirstAlast2Liwickifirst2Mlast3Fern√°ndezfirst3Slast4Bertolamifirst4Rlast5Bunkefirst5Hlast6Schmidhuberfirst6JdateMay 2009titleA Novel Connectionist System for Unconstrained Handwriting Recognitionurlhttpieeexploreieeeorgdocument4531750journalIEEE Transactions on Pattern Analysis and Machine Intelligencevolume31issue5pages855868doi101109tpami2008137issn01628828ref of Alex Graves computer scientistGraves et al won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition ICDAR without any prior knowledge about the three languages to be learnedref namegraves20093ref namegraves 855cite journallast2Liwickifirst2Mlast3Fernandezfirst3Slast4Bertolamifirst4Rlast5Bunkefirst5Hlast6Schmidhuberfirst6Jyear2009titleA Novel Connectionist System for Improved Unconstrained Handwriting RecognitionjournalIEEE Transactions on Pattern Analysis and Machine Intelligencevolume31issue5pages855868doi101109tpami2008137last1Gravesfirst1A url  httpwwwidsiachjuergentpami2008pdf  format  PDFref

Ciresan and colleagues won pattern recognition contests including the IJCNN 2011 Traffic Sign Recognition Competitionref name72Cite journallastCire≈üanfirstDanlast2Meierfirst2Uelilast3Mascifirst3Jonathanlast4Schmidhuberfirst4J√ºrgendateAugust 2012titleMulticolumn deep neural network for traffic sign classificationurlhttpwwwsciencedirectcomsciencearticlepiiS0893608012000524journalNeural NetworksseriesSelected Papers from IJCNN 2011volume32pages333338doi101016jneunet201202023ref the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challengerefCite bookurlhttppapersnipsccpaper4741deepneuralnetworkssegmentneuronalmembranesinelectronmicroscopyimagespdftitleAdvances in Neural Information Processing Systems 25lastCiresanfirstDanlast2Giustifirst2Alessandrolast3Gambardellafirst3Luca Mlast4Schmidhuberfirst4Juergendate2012publisherCurran Associates InceditorlastPereiraeditorfirstFpages28432851editorlast2Burgeseditorfirst2C J Ceditorlast3Bottoueditorfirst3Leditorlast4Weinbergereditorfirst4K Qref and others Their neural networks were the first pattern recognizers to achieve humancompetitive or even superhuman performanceref name92Cite journallastCiresanfirstDanlast2Meierfirst2Ulast3Schmidhuberfirst3JdateJune 2012titleMulticolumn deep neural networks for image classificationurlhttpieeexploreieeeorgdocument6248110journal2012 IEEE Conference on Computer Vision and Pattern Recognitionvolumepages36423649doi101109cvpr20126248110viaisbn9781467312288ref on benchmarks such as traffic sign recognition IJCNN 2012 or the MNIST databaseMNIST handwritten digits problem

Researchers demonstrated 2010 that deep neural networks interfaced to a hidden Markov model with contextdependent states that define the neural network output layer can drastically reduce errors in largevocabulary speech recognition tasks such as voice search

GPUbased implementationsref name6Cite journallastCiresanfirstD Clast2Meierfirst2Ulast3Mascifirst3Jlast4Gambardellafirst4L Mlast5Schmidhuberfirst5Jdate2011editorlasttitleFlexible High Performance Convolutional Neural Networks for Image Classificationurlhttpijcaiorgpapers11PapersIJCAI11210pdfjournalInternational Joint Conference on Artificial Intelligencevolumepagesdoi1055919781577355168ijcai11210viaref of this approach won many pattern recognition contests including the IJCNN 2011 Traffic Sign Recognition Competitionref name72 the ISBI 2012 Segmentation of neuronal structures in EM stacks challengeref name8Cite bookurlhttppapersnipsccpaper4741deepneuralnetworkssegmentneuronalmembranesinelectronmicroscopyimagespdftitleAdvances in Neural Information Processing Systems 25lastCiresanfirstDanlast2Giustifirst2Alessandrolast3Gambardellafirst3Luca Mlast4Schmidhuberfirst4Juergendate2012publisherCurran Associates InceditorlastPereiraeditorfirstFpages28432851editorlast2Burgeseditorfirst2C J Ceditorlast3Bottoueditorfirst3Leditorlast4Weinbergereditorfirst4K Qref the ImageNet Competitionref namekrizhevsky2012cite journallast2Sutskeverfirst2Ilyalast3Hintonfirst3Geoffrydate2012titleImageNet Classification with Deep Convolutional Neural NetworksurlhttpswwwcstorontoedukrizimagenetclassificationwithdeepconvolutionalpdfjournalNIPS 2012 Neural Information Processing Systems Lake Tahoe Nevadalast1Krizhevskyfirst1Alexref and others

Deep highly nonlinear neural architectures similar to the neocognitronref nameK Fukushima Neocognitron 1980cite journalyear1980titleNeocognitron A selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in positionjournalBiological Cyberneticsvolume36issue4pages93202doi101007BF00344251pmid7370364authorFukushima Kref and the standard architecture of visionrefcite journallast2Poggiofirst2Tyear1999titleHierarchical models of object recognition in cortexjournalNature Neurosciencevolume2issue11pages10191025doi10103814819last1Riesenhuberfirst1Mref inspired by Simple cellsimple and complex cells were pretrained by unsupervised methods by Hintonref name1Cite journallastHintonfirstGeoffreydate20090531titleDeep belief networksurlhttpwwwscholarpediaorgarticleDeepbeliefnetworksjournalScholarpedialanguageenvolume4issue5pages5947doi104249scholarpedia5947issn19416016bibcode2009SchpJ45947Hrefref namehinton2006  A team from his lab won a 2012 contest sponsored by Merck  CoMerck to design software to help find molecules that might identify new drugsrefcite newsurlhttpswwwnytimescom20121124sciencescientistsseeadvancesindeeplearningapartofartificialintelligencehtmltitleScientists See Promise in DeepLearning ProgramslastMarkofffirstJohndateNovember 23 2012authornewspaperNew York Timesref

 Convolutional networks 
As of 2011 the state of the art in deep learning feedforward networks alternated convolutional layers and maxpooling layersref name6 ref namemartines2013cite journallast2Bengiofirst2Ylast3Yannakakisfirst3G Nyear2013titleLearning Deep Physiological Models of AffecturljournalIEEE Computational Intelligencevolume8issue2pages2033doi101109mci20132247823last1Martinesfirst1Href topped by several fully or sparsely connected layers followed by a final classification layer Learning is usually done without unsupervised pretraining

Such supervised deep learning methods were the first to achieve humancompetitive performance on certain tasksref name92

ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes only when invariance extended beyond shift to all ANNlearned concepts such as location type object class label scale lighting and others This was realized in Developmental Networks DNsref nameWeng2011J Weng httpwwwcsemsueduwengresearchWhyPassWengNI2011pdf Why Have We Passed Neural Networks Do not Abstract Well Natural Intelligence the INNS Magazine vol 1 no1 pp 1322 2011ref whose embodiments are WhereWhat Networks WWN1 2008ref nameWeng08Z Ji J Weng and D Prokhorov httpwwwcsemsueduwengresearchICDL080077pdf WhereWhat Network 1 Where and What Assist Each Other Through Topdown Connections Proc 7th International Conference on Development and Learning ICDL08 Monterey CA Aug 912 pp 16 2008ref through WWN7 2013ref nameWeng13X Wu G Guo and J Weng httpwwwcsemsueduwengresearchWWN7WuICBM2013pdf Skullclosed Autonomous Development WWN7 Dealing with Scales Proc International Conference on BrainMind July 2728 East Lansing Michigan pp 19 2013ref

Models

ConfusingsectiondateApril 2017
An artificial neural network is a network of simple elements called Artificial neuronneurons which receive input change their internal state activation according to that input and produce output depending on the input and activation The network forms by connecting the output of certain neurons to the input of other neurons forming a Directed graphdirected weighted graph The weights as well as the Activation functionfunctions that compute the activation can be modified by a process called learning which is governed by a learning ruleref nameZell1994ch52cite book lastZell firstAndreas year1994 titleSimulation Neuronaler Netze transtitleSimulation of Neural Networks languageGerman edition1st publisherAddisonWesley  chapterchapter 52 isbn3893195548ref

Components of an artificial neural network

Neurons
A neuron with label mathjmath receiving an input mathpjtmath from predecessor neurons consists of the following componentsref nameZell1994ch52 

 an activation mathajtmath depending on a discrete time parameter
 possibly a threshold maththetajmath which stays fixed unless changed by a learning function
 an activation function mathfmath that computes the new activation at a given time matht1math from mathajtmath maththetajmath and the net input mathpjtmath giving rise to the relation
 math ajt1  fajt pjt thetaj math
 and an output function mathfoutmath computing the output from the activation
 math ojt  foutajt math
Often the output function is simply the Identity function

An input neuron has no predecessor but serves as input interface for the whole network Similarly an output neuron has no successor and thus serves as output interface of the whole network

Connections and weights
The network consists of connections each connection transferring the output of a neuron mathimath to the input of a neuron mathjmath In this sense mathimath is the predecessor of mathjmath and mathjmath is the successor of mathimath Each connection is assigned a weight mathwijmathref nameZell1994ch52 

Propagation function
The propagation function computes the input mathpjtmath to the neuron mathjmath from the outputs mathoitmath of predecessor neurons and typically has the formref nameZell1994ch52 
 math pjt  sumi oit wij math

Learning rule
The learning rule is a rule or an algorithm which modifies the parameters of the neural network in order for a given input to the network to produce a favored output This learning process typically amounts to modifying the weights and thresholds of the variables within the networkref nameZell1994ch52 

Neural networks as functions
See alsoGraphical models

Neural network models can be viewed as simple mathematical models defining a function mathtextstyle  f  X rightarrow Y math or a distribution over mathtextstyle Xmath or both mathtextstyle Xmath and mathtextstyle Ymath Sometimes models are intimately associated with a particular learning rule A common use of the phrase ANN model is really the definition of a class of such functions where members of the class are obtained by varying parameters connection weights or specifics of the architecture such as the number of neurons or their connectivity

Mathematically a neurons network function mathtextstyle fxmath is defined as a composition of other functions mathtextstyle gixmath that can further be decomposed into other functions This can be conveniently represented as a network structure with arrows depicting the dependencies between functions A widely used type of composition is the nonlinear weighted sum where mathtextstyle f x  K leftsumi wi gixright math where mathtextstyle Kmath commonly referred to as the activation functionrefcite weburlhttpwwwcseunsweduaubillwmldicthtmlactivnfntitleThe Machine Learning Dictionaryref is some predefined function such as the hyperbolic functionStandard analytic expressionshyperbolic tangent or sigmoid function or softmax function or ReLUrectifier function The important characteristic of the activation function is that it provides a smooth transition as input values change ie a small change in input produces a small change in output The following refers to a collection of functions mathtextstyle gimath as a Vector mathematics and physicsvector mathtextstyle g  g1 g2 ldots gnmath

FileAnn dependency graphsvgthumb150pxANN dependency graph

This figure depicts such a decomposition of mathtextstyle fmath with dependencies between variables indicated by arrows These can be interpreted in two ways

The first view is the functional view the input mathtextstyle xmath is transformed into a 3dimensional vector mathtextstyle hmath which is then transformed into a 2dimensional vector mathtextstyle gmath which is finally transformed into mathtextstyle fmath This view is most commonly encountered in the context of Mathematical optimizationoptimization

The second view is the probabilistic view the random variable mathtextstyle F  fG math depends upon the random variable mathtextstyle G  gHmath which depends upon mathtextstyle HhXmath which depends upon the random variable mathtextstyle Xmath This view is most commonly encountered in the context of graphical models

The two views are largely equivalent In either case for this particular architecture the components of individual layers are independent of each other eg the components of mathtextstyle gmath are independent of each other given their input mathtextstyle hmath This naturally enables a degree of parallelism in the implementation

FileRecurrent ann dependency graphpngthumb120px Two separate depictions of the recurrent ANN dependency graph

Networks such as the previous one are commonly called feedforward neural networkfeedforward because their graph is a directed acyclic graph Networks with Cycle graph theorycycles are commonly called Recurrent neural networkrecurrent Such networks are commonly depicted in the manner shown at the top of the figure where mathtextstyle fmath is shown as being dependent upon itself However an implied temporal dependence is not shown

Learning
See alsoMathematical optimizationEstimation theoryMachine learning

The possibility of learning has attracted the most interest in neural networks Given a specific task to solve and a class of functions mathtextstyle Fmath learning means using a set of observations to find mathtextstyle  f in Fmath which solves the task in some optimal sense

This entails defining a cost function mathtextstyle C  F rightarrow mathbbRmath such that for the optimal solution mathtextstyle fmath mathtextstyle Cf leq Cfmath mathtextstyle forall f in Fmathsnd ie no solution has a cost less than the cost of the optimal solution see mathematical optimization

The cost function mathtextstyle Cmath is an important concept in learning as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved Learning algorithms search through the solution space to find a function that has the smallest possible cost

For applications where the solution is data dependent the cost must necessarily be a function of the observations otherwise the model would not relate to the data It is frequently defined as a statistic to which only approximations can be made As a simple example consider the problem of finding the model mathtextstyle fmath which minimizes mathtextstyle CEleftfx  y2rightmath for data pairs mathtextstyle xymath drawn from some distribution mathtextstyle mathcalDmath In practical situations we would only have mathtextstyle Nmath samples from mathtextstyle mathcalDmath and thus for the above example we would only minimize mathtextstyle hatCfrac1Nsumi1N fxiyi2math Thus the cost is minimized over a sample of the data rather than the entire distribution

When mathtextstyle N rightarrow inftymath some form of online machine learning must be used where the cost is reduced as each new example is seen While online machine learning is often used when mathtextstyle mathcalDmath is fixed it is most useful in the case where the distribution changes slowly over time In neural network methods some form of online machine learning is frequently used for finite datasets

Choosing a cost function
While it is possible to define an ad hoc cost function frequently a particular cost function is used either because it has desirable properties such as Convex functionconvexity or because it arises naturally from a particular formulation of the problem eg in a probabilistic formulation the posterior probability of the model can be used as an inverse cost Ultimately the cost function depends on the task

Backpropagation
MainBackpropagation
A Deep neural networkDNN can be Discriminative modeldiscriminatively trained with the standard backpropagation algorithm Backpropagationnbspis a method to calculate thenbspgradientnbspof thenbsploss functionnbspproduces the cost associated with a given state with respect to the weights in annbspANN

The basics of continuous backpropagationref nameSCHIDHUB2ref namescholarpedia2cite journalyear2015titleDeep LearningurlhttpwwwscholarpediaorgarticleDeepLearningjournalScholarpediavolume10issue11page32832doi104249scholarpedia32832last1Schmidhuberfirst1J√ºrgenauthorlinkJ√ºrgen Schmidhuberbibcode2015SchpJ1032832Srefref name5Cite journallastDreyfusfirstStuart Edate19900901titleArtificial neural networks back propagation and the KelleyBryson gradient procedureurlhttparcaiaaorgdoi102514325422journalJournal of Guidance Control and Dynamicsvolume13issue5pages926928doi102514325422issn07315090bibcode1990JGCD13926Drefref namemizutani2000Eiji Mizutani Stuart Dreyfus Kenichi Nishio 2000 On derivation of MLP backpropagation from the KelleyBryson optimalcontrol gradient formula and its application Proceedings of the IEEE International Joint Conference on Neural Networks IJCNN 2000 Como Italy July 2000 httpqueueieorberkeleyeduPeopleFacultydreyfuspubsijcnn2kpdf Onlineref were derived in the context of control theory by Henry J KelleyKelleyref namekelley1960cite journalyear1960titleGradient theory of optimal flight pathsurlhttparcaiaaorgdoiabs10251485282journalCodearsjjournalArs Journalvolume30issue10pages947954doi10251485282last1Kelleyfirst1Henry JauthorlinkHenry J Kelleyref in 1960 and by Arthur E BrysonBryson in 1961ref namebryson1961Arthur E Bryson 1961 April A gradient method for optimizing multistage allocation processes In Proceedings of the Harvard Univ Symposium on digital computers and their applicationsref using principles of dynamic programming In 1962 Stuart DreyfusDreyfus published a simpler derivation based only on the chain ruleref namedreyfus1962cite journalyear1962titleThe numerical solution of variational problemsurlhttpswwwresearchgatenetpublication256244271ThenumericalsolutionofvariationalproblemsjournalJournal of Mathematical Analysis and Applicationsvolume5issue1pages3045doi1010160022247x62900045last1Dreyfusfirst1StuartauthorlinkStuart Dreyfusref Bryson and YuChi HoHo described it as a multistage dynamic system optimization method in 1969refcite bookurlgoogle books plainurly id8jZBkshbUMCpage578titleArtificial Intelligence A Modern Approachlast2Norvigfirst2PeterpublisherPrentice Hallyear2010isbn9780136042594page578quoteThe most popular method for learning in multilayer networks is called Backpropagationauthorlink2Peter Norvigfirst1Stuart Jlast1Russellauthorlink1Stuart J Russellrefref nameBryson1969cite bookurlgoogle books plainurly id1bChDAEACAAJpage481titleApplied Optimal Control Optimization Estimation and ControllastBrysonfirstArthur EarlpublisherBlaisdell Publishing Company or Xerox College Publishingyear1969page481ref In 1970 Seppo LinnainmaaLinnainmaa finally published the general method for automatic differentiation AD of discrete connected networks of nested Differentiable functiondifferentiable functionsref namelin1970Seppo Linnainmaa 1970 The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors Masters Thesis in Finnish Univ Helsinki 67refref namelin1976cite journalyear1976titleTaylor expansion of the accumulated rounding errorurljournalBIT Numerical Mathematicsvolume16issue2pages146160doi101007bf01931367last1Linnainmaafirst1SeppoauthorlinkSeppo Linnainmaaref This corresponds to the modern version of backpropagation which is efficient even when the networks are sparseref nameSCHIDHUB2ref namescholarpedia2ref namegrie2012Cite journallastGriewankfirstAndreasdate2012titleWho Invented the Reverse Mode of Differentiationurlhttpwwwmathuiucedudocumentavolismp52griewankandreasbpdfjournalDocumenta Matematica Extra Volume ISMPvolumepages389400viarefref namegrie2008cite bookurlgoogle books plainurly idxoiiLaRxcbECtitleEvaluating Derivatives Principles and Techniques of Algorithmic Differentiation Second Editionlast2Waltherfirst2AndreapublisherSIAMyear2008isbn9780898717761first1Andreaslast1Griewankref In 1973 Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradientsref namedreyfus1973cite journalyear1973titleThe computational solution of optimal control problems with time lagurljournalIEEE Transactions on Automatic Controlvolume18issue4pages383385doi101109tac19731100330last1Dreyfusfirst1StuartauthorlinkStuart Dreyfusref In 1974 Paul WerbosWerbos mentioned the possibility of applying this principle to ANNsref namewerbos1974Paul Werbos 1974 Beyond regression New tools for prediction and analysis in the behavioral sciences PhD thesis Harvard Universityref and in 1982 he applied Linnainmaas AD method to neural networks in the way that is widely used todayref namescholarpedia2ref namewerbos1982Cite bookurlhttpwerboscomNeuralSensitivityIFIPSeptember1981pdftitleSystem modeling and optimizationlastWerbosfirstPaulauthorlinkPaul WerbospublisherSpringeryear1982isbnlocationpages762770chapterApplications of advances in nonlinear sensitivity analysisref In 1986 David E RumelhartRumelhart Hinton and Ronald J WilliamsWilliams noted that this method can generate useful internal representations of incoming data in hidden layers of neural networksref name4Cite journallastRumelhartfirstDavid Elast2Hintonfirst2Geoffrey Elast3Williamsfirst3Ronald JtitleLearning representations by backpropagating errorsurlhttpwwwnaturecomarticlesArt323533a0journalNaturevolume323issue6088pages533536doi101038323533a0year1986bibcode1986Natur323533Rref In 1993 Wan was the firstref nameSCHIDHUB2 to win an international pattern recognition contest through backpropagationref namewan1993Eric A Wan 1993 Time series prediction by using a connectionist network with internal delay lines In SANTA FE INSTITUTE STUDIES IN THE SCIENCES OF COMPLEXITYPROCEEDINGS Vol 15 pp 195195 AddisonWesley Publishing Coref

The weight updates of backpropagation can be done via stochastic gradient descent using the following equation
 math wijt  1  wijt  etafracpartial Cpartial wij xit math
where math eta math is the learning rate math C math is the cost loss function and mathxitmath a stochastic term The choice of the cost function depends on factors such as the learning type supervised unsupervised Reinforcement learningreinforcement etc and the activation function For example when performing supervised learning on a multiclass classification problem common choices for the activation function and cost function are the Softmax activation functionsoftmax function and cross entropy function respectively The softmax function is defined as math pj  fracexpxjsumk expxk math where math pj math represents the class probability output of the unit math j math and math xj math and math xk math represent the total input to units math j math and math k math of the same level respectively Cross entropy is defined as math C  sumj dj logpj math where math dj math represents the target probability for output unit math j math and math pj math is the probability output for math j math after applying the activation functionrefCite journallastHintonfirstGlast2Dengfirst2Llast3Yufirst3Dlast4Dahlfirst4G Elast5Mohamedfirst5A rlast6Jaitlyfirst6Nlast7Seniorfirst7Alast8Vanhouckefirst8Vlast9Nguyenfirst9PdateNovember 2012titleDeep Neural Networks for Acoustic Modeling in Speech Recognition The Shared Views of Four Research Groupsurlhttpieeexploreieeeorgdocument6296526journalIEEE Signal Processing Magazinevolume29issue6pages8297doi101109msp20122205597issn10535888bibcode2012ISPM2982Href

These can be used to output object Minimum bounding boxbounding boxes in the form of a binary mask They are also used for multiscale regression to increase localization precision DNNbased regression can learn features that capture geometric information in addition to serving as a good classifier They remove the requirement to explicitly model parts and their relations This helps to broaden the variety of objects that can be learned The model consists of multiple layers each of which has a rectified linear unit as its activation function for nonlinear transformation Some layers are convolutional while others are fully connected Every convolutional layer has an additional max pooling The network is trained to minimize L2 normLsup2sup error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks

Alternatives to backpropagation include Extreme Learning Machinesrefcite journallast2Zhufirst2QinYulast3Siewfirst3CheeKheongyear2006titleExtreme learning machine theory and applicationsurljournalNeurocomputingvolume70issue1pages489501doi101016jneucom200512126last1Huangfirst1GuangBinref Noprop networksrefcite journalyear2013titleThe noprop algorithm A new learning algorithm for multilayer neural networksurljournalNeural Networksvolume37issuepages182188doi101016jneunet201209020last1Widrowfirst1Bernarddisplayauthorsetalref training without backtrackingrefcite arXiveprint150707680firstYannlastOllivierfirst2Guillaumelast2CharpiattitleTraining recurrent networks without backtrackingyear2015classcsNEref weightless networksrefESANN 2009refref nameRBMTRAINCite journallastHintonfirstG Edate2010titleA Practical Guide to Training Restricted Boltzmann Machinesurlhttpswwwresearchgatenetpublication221166159AbriefintroductiontoWeightlessNeuralSystemsjournalTech Rep UTML TR 2010003volumepagesviaref and Holographic associative memorynonconnectionist neural networks

Learning paradigms
The three major learning paradigms each correspond to a particular learning task These are supervised learning unsupervised learning and reinforcement learning

 Supervised learning 
Supervised learning uses a set of example pairs math x y x in X y in Ymath and the aim is to find a function math  f  X rightarrow Y math in the allowed class of functions that matches the examples In other words we wish to infer the mapping implied by the data the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domainrefCite journallastOjhafirstVarun Kumarlast2Abrahamfirst2Ajithlast3Sn√°≈°elfirst3V√°clavdate20170401titleMetaheuristic design of feedforward neural networks A review of two decades of researchurlhttpwwwsciencedirectcomsciencearticlepiiS0952197617300234journalEngineering Applications of Artificial Intelligencevolume60pages97116doi101016jengappai201701013ref

A commonly used cost is the meansquared error which tries to minimize the average squared error between the networks output math fxmath and the target value math ymath over all the example pairs Minimizing this cost using gradient descent for the class of neural networks called multilayer perceptrons MLP produces the Backpropagationbackpropagation algorithm for training neural networks

Tasks that fall within the paradigm of supervised learning are pattern recognition also known as classification and Regression analysisregression also known as function approximation The supervised learning paradigm is also applicable to sequential data eg for hand writing speech and gesture recognition This can be thought of as learning with a teacher in the form of a function that provides continuous feedback on the quality of solutions obtained thus far

Unsupervised learning
In unsupervised learning some data mathtextstyle xmath is given and the cost function to be minimized that can be any function of the data mathtextstyle xmath and the networks output mathtextstyle fmath

The cost function is dependent on the task the model domain and any A priori and a posterioria priori assumptions the implicit properties of the model its parameters and the observed variables

As a trivial example consider the model mathtextstyle fx  amath where mathtextstyle amath is a constant and the cost mathtextstyle CEx  fx2math Minimizing this cost produces a value of mathtextstyle amath that is equal to the mean of the data The cost function can be much more complicated Its form depends on the application for example in Data compressioncompression it could be related to the mutual information between mathtextstyle xmath and mathtextstyle fxmath whereas in statistical modeling it could be related to the posterior probability of the model given the data note that in both of those examples those quantities would be maximized rather than minimized

Tasks that fall within the paradigm of unsupervised learning are in general Approximationestimation problems the applications include Data clusteringclustering the estimation of statistical distributions Data compressioncompression and Bayesian spam filteringfiltering

Reinforcement learning
See alsoStochastic control

In reinforcement learning data mathtextstyle xmath are usually not given but generated by an agents interactions with the environment At each point in time mathtextstyle tmath the agent performs an action mathtextstyle ytmath and the environment generates an observation mathtextstyle xtmath and an instantaneous cost mathtextstyle ctmath according to some usually unknown dynamics The aim is to discover a policy for selecting actions that minimizes some measure of a longterm cost eg the expected cumulative cost The environments dynamics and the longterm cost for each policy are usually unknown but can be estimated

More formally the environment is modeled as a Markov decision process MDP with states mathtextstyle s1snin S math and actions mathtextstyle a1am in Amath with the following probability distributions the instantaneous cost distribution mathtextstyle Pctstmath the observation distribution mathtextstyle Pxtstmath and the transition mathtextstyle Pst1st atmath while a policy is defined as the conditional distribution over actions given the observations Taken together the two then define a Markov chain MC The aim is to discover the policy ie the MC that minimizes the cost

ANNs are frequently used in reinforcement learning as part of the overall algorithmrefcite conference author  Dominic S author2Das R author3Whitley D author4Anderson C dateJuly 1991  title  Genetic reinforcement learning for neural networks  conference  IJCNN91Seattle International Joint Conference on Neural Networks  booktitle  IJCNN91Seattle International Joint Conference on Neural Networks  publisher  IEEE  location  Seattle Washington USA   url  httpsdxdoiorg101109IJCNN1991155315  doi  101109IJCNN1991155315  accessdate  29 July 2012  isbn  0780301641 refrefcite journal lastHoskins firstJC author2Himmelblau DM titleProcess control via artificial neural networks and reinforcement learning journalComputers  Chemical Engineering year1992 volume16 pages241251 doi101016009813549280045B issue4ref Dynamic programming was coupled with ANNs giving neurodynamic programming by Dimitri BertsekasBertsekas and Tsitsiklisrefcite bookurlhttpspapersnipsccpaper4741deepneuralnetworkssegmentneuronalmembranesinelectronmicroscopyimagestitleNeurodynamic programmingfirstDPfirst2JNpublisherAthena Scientificyear1996isbn1886529108locationpage512pagesauthorBertsekasauthor2Tsitsiklisref and applied to multidimensional nonlinear problems such as those involved in vehicle routingrefcite journal lastSecomandi firstNicola titleComparing neurodynamic programming algorithms for the vehicle routing problem with stochastic demands journalComputers  Operations Research year2000 volume27 pages12011225 doi101016S030505489900146X issue1112ref natural resource managementnatural resources managementrefcite conference author  de Rigo D author2Rizzoli A E author3SonciniSessa R author4Weber E author5Zenesi P  year  2001  title  Neurodynamic programming for the efficient management of reservoir networks  conference  MODSIM 2001 International Congress on Modelling and Simulation  conferenceurl  httpwwwmssanzorgauMODSIM01MODSIM01htm  booktitle  Proceedings of MODSIM 2001 International Congress on Modelling and Simulation  publisher  Modelling and Simulation Society of Australia and New Zealand  location  Canberra Australia  doi  105281zenodo7481  url  httpszenodoorgrecord7482filesdeRigoetalMODSIM2001activelinkauthorcopypdf  accessdate  29 July 2012  isbn  0867405252 refrefcite conference author  Damas M author2Salmeron M author3Diaz A author4Ortega J author5Prieto A author6Olivares G year  2000  title  Genetic algorithms and neurodynamic programming application to water supply networks  conference  2000 Congress on Evolutionary Computation  booktitle  Proceedings of 2000 Congress on Evolutionary Computation  publisher  IEEE  location  La Jolla California USA  url  httpsdxdoiorg101109CEC2000870269  doi  101109CEC2000870269  accessdate  29 July 2012  isbn  0780363752  ref or medicinerefcite journal lastDeng firstGeng author2Ferris MC titleNeurodynamic programming for fractionated radiotherapy planning journalSpringer Optimization and Its Applications year2008 volume12 pages4770 doi10100797803877329923citeseerx10111378288 seriesSpringer Optimization and Its Applications isbn9780387732985 ref because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems

Tasks that fall within the paradigm of reinforcement learning are control problems games and other sequential decision making tasks

 Convergent recursive learning algorithm 
This is a learning  method specially designed for cerebellar model articulation controller CMAC neural networks In 2004 a recursive least squares algorithm was introduced to train cerebellar model articulation controllerCMAC neural network onlineref nameQin1Ting Qin  et al A learning algorithm of CMAC based on RLS Neural Processing Letters 191 2004 4961ref This algorithm can converge in one step and update all weights in one step with any new input data  Initially this algorithm had Computational complexity theorycomputational complexity of ONsup3sup Based on QR decomposition this recursive learning algorithm was simplified to be ONref nameQin2Ting Qin et al Continuous CMACQRLS and its systolic array Neural Processing Letters 221 2005 116ref

Learning algorithms
See alsoMachine learning

Training a neural network model essentially means selecting one model from the set of allowed models or in a Bayesian probabilityBayesian framework determining a distribution over the set of allowed models that minimizes the cost Numerous algorithms are available for training neural network models most of them can be viewed as a straightforward application of Mathematical optimizationoptimization theory and statistical estimation

Most employ some form of gradient descent using backpropagation to compute the actual gradients This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradientrelated direction Backpropagation training algorithms fall into three categories 
 Gradient descentsteepest descent with variable learning rate and Gradient descentThe momentum methodmomentum Rpropresilient backpropagation
 quasiNewton BroydenFletcherGoldfarbShanno algorithmBroydenFletcherGoldfarbShanno Secant methodone step secant
 LevenbergMarquardt algorithmLevenbergMarquardt and Conjugate gradient methodconjugate gradient FletcherReeves update PolakRibi√©re update PowellBeale restart scaled conjugate gradientrefcite conferenceauthor1M Forouzanfar author2H R Dajani author3V Z Groza author4M Bolic author5S Rajan  lastauthorampyes dateJuly 2010  title  Comparison of FeedForward Neural Network Training Algorithms for Oscillometric Blood Pressure Estimation  conference  4th Int Workshop Soft Computing Applications  publisher  IEEE location  Arad Romania urlhttpswwwresearchgatenetprofileMohamadForouzanfarpublication224173336ComparisonofFeedForwardNeuralNetworktrainingalgorithmsforoscillometricbloodpressureestimationlinks00b7d533829c3a7484000000pdfevpubintdocdloriginpublicationdetailinViewertruemsrpTyT962BjWOHJo2BVhkMF4IzwHPAImSd442n2BAkEuXj9qBmQSZ495CpxqlaOYon2BSlEzWQElBGyJmbBCiiUOV8ImeEqPFXiIRivcrWsWmlPBYU3D ref

Evolutionary methodsrefcite conference authors  de Rigo D Castelletti A Rizzoli AE SonciniSessa R Weber E dateJanuary 2005  title  A selective improvement technique for fastening NeuroDynamic Programming in Water Resources Network Management  conference  16th IFAC World Congress  conferenceurl  httpwwwntntnunousersskogeprostproceedingsifac2005Indexhtml  booktitle  Proceedings of the 16th IFAC World Congress  IFACPapersOnLine  editor  Pavel Z√≠tek  volume  16  publisher  IFAC  location  Prague Czech Republic  url  httpwwwntntnunousersskogeprostproceedingsifac2005PapersPaper4269html
  accessdate  30 December 2011  doi  103182200507036CZ190202172  isbn  9783902661753 ref gene expression programmingrefcite weblastFerreirafirstCyear2006titleDesigning Neural Networks Using Gene Expression Programmingurl httpwwwgeneexpressionprogrammingcomwebpapersFerreiraASCT2006pdfpublisher In A Abraham B de Baets M K√∂ppen and B Nickolay eds Applied Soft Computing Technologies The Challenge of Complexity pages 517536 SpringerVerlagref simulated annealingrefcite conference author  Da Y author2Xiurun G dateJuly 2005  title  An improved PSObased ANN with simulated annealing technique  conference  New Aspects in Neurocomputing 11th European Symposium on Artificial Neural Networks  conferenceurl  httpwwwdiceuclacbeesannproceedingselectronicproceedingshtm  editor  T Villmann  publisher  Elsevier  doi  101016jneucom200407002  accessdate  30 December 2011 ref expectationmaximization nonparametric methods and particle swarm optimizationrefcite conference author  Wu J author2Chen E dateMay 2009  title  A Novel Nonparametric Regression Ensemble for Rainfall Forecasting Using Particle Swarm Optimization Technique Coupled with Artificial Neural Network  conference  6th International Symposium on Neural Networks ISNN 2009  conferenceurl  httpwww2maecuhkeduhkisnn2009  editors  Wang H Shen Y Huang T Zeng Z  publisher  Springer  doi  10100797836420151376  isbn  9783642012150  accessdate  1 January 2012 ref are other methods for training neural networks

 Variants 

 Group method of data handling 
MainGroup method of data handlingThe Group Method of Data Handling GMDHref nameivak1968cite journalyear1968titleThe group method of data handling  a rival of the method of stochastic approximationurljournalSoviet Automatic Controlvolume13issue3pages4355last1Ivakhnenkofirst1Alexey GrigorevichauthorlinkAlexey Grigorevich Ivakhnenkoref features fully automatic structural and parametric model optimization The node activation functions are Andrey KolmogorovKolmogorovGabor polynomials that permit additions and multiplications It used a deep feedforward multilayer perceptron with eight layersref nameivak1971Cite journallastIvakhnenkofirstAlexeydate1971titlePolynomial theory of complex systemsurljournalIEEE Transactions on Systems Man and Cybernetics 4issue4pages364378doi101109TSMC19714308320pmidaccessdateref It is a supervised learning network that grows layer by layer where each layer is trained by regression analysis Useless items are detected using a validation set and pruned through Regularization mathematicsregularization The size and depth of the resulting network depends on the taskref namekondo2008cite journallast2Uenofirst2Jdateyear2008titleMultilayered GMDHtype neural network selfselecting optimum neural network architecture and its application to 3dimensional medical image recognition of blood vesselsurlhttpswwwresearchgatenetpublication228402366GMDHTypeNeuralNetworkSelfSelectingOptimumNeuralNetworkArchitectureandItsApplicationto3DimensionalMedicalImageRecognitionoftheLungsjournalInternational Journal of Innovative Computing Information and Controlvolume4issue1pages175187vialast1Kondofirst1Tref

 Convolutional neural networks 
mainConvolutional neural networkA convolutional neural network CNN is a class of deep feedforward networks composed of one or more convolutional layers with fully connected layers matching those in typical ANNs on top It uses tied weights and pooling layers In particular maxpoolingref nameWeng19932 is often structured via Fukushimas convolutional architectureref nameFUKU1980cite journalyear1980titleNeocognitron A selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in positionurljournalBiol Cybernvolume36issue4pages193202doi101007bf00344251pmid7370364last1Fukushimafirst1Kref This architecture allows CNNs to take advantage of the 2D structure of input data

CNNs are suitable for processing visual and other twodimensional dataref nameLECUN1989LeCun et al Backpropagation Applied to Handwritten Zip Code Recognition Neural Computation 1 pp 541551 1989refref namelecun2016slidesYann LeCun 2016 Slides on Deep Learning httpsindicocernchevent510372 Onlineref They have shown superior results in both image and speech applications They can be trained with standard backpropagation CNNs are easier to train than other regular deep feedforward neural networks and have many fewer parameters to estimateref nameSTANCNNcite weburlhttpufldlstanfordedututorialsupervisedConvolutionalNeuralNetworktitleUnsupervised Feature Learning and Deep Learning Tutorialpublisherref Examples of applications in computer vision include DeepDream ref namedeepdreamcite journallast2Liufirst2Weilast3Jiafirst3Yangqinglast4Sermanetfirst4Pierrelast5Reedfirst5Scottlast6Anguelovfirst6Dragomirlast7Erhanfirst7Dumitrulast8Vanhouckefirst8Vincentlast9Rabinovichfirst9Andrewdateyear2014titleGoing Deeper with ConvolutionsurljournalComputing Research Repositoryvolumepages1arxiv14094842doi101109CVPR20157298594viafirst1Christianlast1Szegedyisbn9781467369640ref and robot navigationrefcite journal  lastRan  firstLingyan  last2Zhang  first2Yanning  last3Zhang  first3Qilin  last4Yang  first4Tao  titleConvolutional Neural NetworkBased Robot Navigation Using Uncalibrated Spherical Images  journalSensors  publisherMDPI AG  volume17  issue6  date20170612  issn14248220  doi103390s17061341  page1341  urlhttpsqilinzhanggithubiopagespdfssensors1701341pdfref

 Long shortterm memory 
mainLong shortterm memoryLong shortterm memory LSTM networks are RNNs that avoid the vanishing gradient problemref name03Cite journallastHochreiterfirstSeppauthorlinkSepp Hochreiterlast2Schmidhuberfirst2J√ºrgenauthorlink2J√ºrgen Schmidhuberdate19971101titleLong ShortTerm Memoryurlhttpwwwmitpressjournalsorgdoi101162neco1997981735journalNeural Computationvolume9issue8pages17351780doi101162neco1997981735issn08997667viaref LSTM is normally augmented by recurrent gates called forget gatesref name10Cite weburlhttpswwwresearchgatenetpublication220320057LearningPreciseTimingwithLSTMRecurrentNetworkstitleLearning Precise Timing with LSTM Recurrent Networks PDF Download AvailablewebsiteResearchGatelanguageenaccessdate20170613pp115143ref LSTM networks prevent backpropagated errors from vanishing or explodingref nameHOCH19912 Instead errors can flow backwards through unlimited numbers of virtual layers in spaceunfolded LSTM That is LSTM can learn very deep learning tasksref nameSCHIDHUB2  that require memories of events that happened thousands or even millions of discrete time steps ago Problemspecific LSTMlike topologies can be evolvedrefCite journallastBayerfirstJustinlast2Wierstrafirst2Daanlast3Togeliusfirst3Julianlast4Schmidhuberfirst4J√ºrgendate20090914titleEvolving Memory Cell Structures for Sequence Learningurlhttpslinkspringercomchapter101007978364204277576journalArtificial Neural Networks  ICANN 2009volume5769languageenpublisherSpringer Berlin Heidelbergpages755764doi101007978364204277576seriesLecture Notes in Computer Scienceisbn9783642042768ref LSTM can handle long delays and signals that have a mix of low and high frequency components

Stacks of LSTM RNNsrefCite journallastFern√°ndezfirstSantiagolast2Gravesfirst2Alexlast3Schmidhuberfirst3J√ºrgendate2007titleSequence labelling in structured domains with hierarchical recurrent neural networksurlhttpciteseerxistpsueduviewdocsummarydoi1011791887journalIn Proc 20th Int Joint Conf on Artificial Inligence Ijcai 2007pages774779ref trained by Connectionist Temporal Classification CTCref name12Cite journallastGravesfirstAlexlast2Fern√°ndezfirst2Santiagolast3Gomezfirst3Faustinodate2006titleConnectionist temporal classification Labelling unsegmented sequence data with recurrent neural networksurlhttpciteseerxistpsueduviewdocsummarydoi1011756306journalIn Proceedings of the International Conference on Machine Learning ICML 2006pages369376ref can find an RNN weight matrix that maximizes the probability of the label sequences in a training set given the corresponding input sequences CTC achieves both alignment and recognition

In 2003 LSTM started to become competitive with traditional speech recognizersref namegraves2003Cite weburlFtpftpidsiachpubjuergenbioadit2004pdftitleBiologically Plausible Speech Recognition with LSTM Neural NetslastGravesfirstAlexlast2Eckfirst2Douglasdate2003website1st Intl Workshop on Biologically Inspired Approaches to Advanced Information Technology BioADIT 2004 Lausanne Switzerlandpages175184archiveurlarchivedatedeadurlaccessdatelast3Beringerfirst3Nicolelast4Schmidhuberfirst4J√ºrgenauthorlink4J√ºrgen Schmidhuberref In 2007 the combination with CTC achieved first good results on speech dataref namefernandez2007keywordCite journallastFern√°ndezfirstSantiagolast2Gravesfirst2Alexlast3Schmidhuberfirst3J√ºrgendate2007titleAn Application of Recurrent Neural Networks to Discriminative Keyword Spottingurlhttpdlacmorgcitationcfmid17780661778092journalProceedings of the 17th International Conference on Artificial Neural NetworksseriesICANN07locationBerlin HeidelbergpublisherSpringerVerlagpages220229isbn3540746935ref In 2009 a CTCtrained LSTM was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognitionref nameSCHIDHUB2 ref namegraves20093 In 2014 Baidu used CTCtrained RNNs to break the Switchboard Hub500 speech recognition benchmark without traditional speech processing methodsref namehannun2014cite arxivlastHannunfirstAwnilast2Casefirst2Carllast3Casperfirst3Jaredlast4Catanzarofirst4Bryanlast5Diamosfirst5Greglast6Elsenfirst6Erichlast7Prengerfirst7Ryanlast8Satheeshfirst8Sanjeevlast9Senguptafirst9Shubhodate20141217titleDeep Speech Scaling up endtoend speech recognitioneprint14125567classcsCLref LSTM also improved largevocabulary speech recognitionref namesak2014Cite weburlhttpsstaticgoogleusercontentcommediaresearchgooglecomenpubsarchive43905pdftitleLong ShortTerm Memory recurrent neural network architectures for large scale acoustic modelinglastSakfirstHasimlast2Seniorfirst2Andrewdate2014websitearchiveurlarchivedatedeadurlaccessdatelast3Beaufaysfirst3Francoiserefref nameliwu2015cite arxivlastLifirstXianganglast2Wufirst2Xihongdate20141015titleConstructing Long ShortTerm Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognitioneprint14104281classcsCLref texttospeech synthesisrefCite weburlhttpswwwresearchgatenetpublication287741874TTSsynthesiswithbidirectionalLSTMbasedRecurrentNeuralNetworkstitleTTS synthesis with bidirectional LSTM based Recurrent Neural NetworkslastFanfirstYlast2Qianfirst2Ydate2014websiteResearchGatelanguageenarchiveurlarchivedatedeadurlaccessdate20170613last3Xiefirst3Flast4Soongfirst4F Kref for Google Androidref namescholarpedia2ref namezen2015Cite weburlhttpsstaticgoogleusercontentcommediaresearchgooglecomenpubsarchive43266pdftitleUnidirectional Long ShortTerm Memory Recurrent Neural Network with Recurrent Output Layer for LowLatency Speech SynthesislastZenfirstHeigalast2Sakfirst2Hasimdate2015websiteGooglecompublisherICASSPpages44704474archiveurlarchivedatedeadurlaccessdateref and photoreal talking headsref namefan2015Cite journallastFanfirstBolast2Wangfirst2Lijuanlast3Soongfirst3Frank Klast4Xiefirst4Leidate2015titlePhotoReal Talking Head with Deep Bidirectional LSTMurlhttpswwwmicrosoftcomenusresearchwpcontentuploads201504icassp2015fanbo1009pdfjournalProceedings of ICASSPvolumepagesviaref In 2015 Googles speech recognition experienced a 49 improvement through CTCtrained LSTMref namesak2015Cite weburlhttpgoogleresearchblogspotch201509googlevoicesearchfasterandmorehtmltitleGoogle voice search faster and more accuratelastSakfirstHa≈üimlast2Seniorfirst2AndrewdateSeptember 2015websitearchiveurlarchivedatedeadurlaccessdatelast3Raofirst3Kanishkalast4Beaufaysfirst4Fran√ßoiselast5Schalkwykfirst5Johanref

LSTM became popular in Natural Language Processing Unlike previous models based on Hidden Markov modelHMMs and similar concepts LSTM can learn to recognise contextsensitive languagesref namegers2001cite journallast2Schmidhuberfirst2J√ºrgenyear2001titleLSTM Recurrent Networks Learn Simple Context Free and Context Sensitive LanguagesurljournalIEEE Transactions on Neural Networksvolume12issue6pages13331340doi10110972963769last1Gersfirst1Felix Aauthorlink2J√ºrgen Schmidhuberref LSTM improved machine translation refcite web  lastHuang  firstJie  last2Zhou  first2Wengang  last3Zhang  first3Qilin  last4Li  first4Houqiang  last5Li  first5Weiping  titleVideobased Sign Language Recognition without Temporal Segmentation  websitearXivorg ePrint archive  date20180130  urlhttpsarxivorgpdf180110111pdfrefref nameNIPS2014Cite journallastSutskeverfirstLlast2Vinyalsfirst2Olast3Lefirst3Qdate2014titleSequence to Sequence Learning with Neural Networksurlhttpspapersnipsccpaper5346sequencetosequencelearningwithneuralnetworkspdfjournalNIPS14 Proceedings of the 27th International Conference on Neural Information Processing Systems volume2 pages31043112 bibcode2014arXiv14093215S arxiv14093215 classcsCLref language modelingref namevinyals2016cite arxivlastJozefowiczfirstRafallast2Vinyalsfirst2Oriollast3Schusterfirst3Mikelast4Shazeerfirst4Noamlast5Wufirst5Yonghuidate20160207titleExploring the Limits of Language Modelingeprint160202410classcsCLref and multilingual language processingref namegillick2015cite arxivlastGillickfirstDanlast2Brunkfirst2Clifflast3Vinyalsfirst3Oriollast4Subramanyafirst4Amarnagdate20151130titleMultilingual Language Processing From Byteseprint151200103classcsCLref LSTM combined with CNNs improved automatic image captioningref namevinyals2015cite arxivlastVinyalsfirstOriollast2Toshevfirst2Alexanderlast3Bengiofirst3Samylast4Erhanfirst4Dumitrudate20141117titleShow and Tell A Neural Image Caption Generatoreprint14114555classcsCVref

 Deep reservoir computing 
MainReservoir computingDeep Reservoir Computing and Deep Echo State Networks deepESNsrefCite journallastGallicchiofirstClaudiolast2Michelifirst2Alessiolast3Pedrellifirst3LucatitleDeep reservoir computing A critical experimental analysisurlhttpwwwsciencedirectcomsciencearticlepiiS0925231217307567journalNeurocomputingvolume268pages87doi101016jneucom201612089year2017refrefCite journallastGallicchiofirstClaudiolast2Michelifirst2AlessiodatetitleEcho State Property of Deep Reservoir Computing Networksurlhttpslinkspringercomarticle101007s1255901794619journalCognitive Computationlanguageenvolume9issue3pages337350doi101007s1255901794619issn18669956viayear2017ref provide a framework for efficiently trained modelsnbspfor hierarchical processing of temporal data while enabling the investigation of the inherent role of RNN layered compositionclarifydateJanuary 2018

 Deep belief networks 
mainDeep belief network
FileRestrictedBoltzmannmachinesvglinkhttpsenwikipediaorgwikiFileRestrictedBoltzmannmachinesvgthumbA restricted Boltzmann machine RBM with fully connected visible and hidden units Note there are no hiddenhidden or visiblevisible connections
A deep belief network DBN is a probabilistic generative model made up of multiple layers of hidden units It can be considered a Function compositioncomposition of simple learning modules that make up each layerref nameSCHOLARDBNScite journalyear2009titleDeep belief networksurljournalScholarpediavolume4issue5page5947doi104249scholarpedia5947last1Hintonfirst1GEbibcode2009SchpJ45947Href

A DBN can be used to generatively pretrain a DNN by using the learned DBN weights as the initial DNN weights Backpropagation or other discriminative algorithms can then tune these weights This is particularly helpful when training data are limited because poorly initialized weights can significantly hinder model performance These pretrained weights are in a region of the weight space that is closer to the optimal weights than were they randomly chosen This allows for both improved modeling and faster convergence of the finetuning phaserefCite journallastLarochellefirstHugolast2Erhanfirst2Dumitrulast3Courvillefirst3Aaronlast4Bergstrafirst4Jameslast5Bengiofirst5Yoshuadate2007titleAn Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variationurlhttpdoiacmorg10114512734961273556journalProceedings of the 24th International Conference on Machine LearningseriesICML 07locationNew York NY USApublisherACMpages473480doi10114512734961273556isbn9781595937933ref

 Large memory storage and retrieval neural networks 
Large memory storage and retrieval neural networks LAMSTARref namebook2013cite bookurlgoogle books plainurly idW6W6CgAAQBAJpgPP1titlePrinciples of Artificial Neural NetworkslastGraupefirstDanielpublisherWorld Scientificyear2013isbn9789814522748locationpages1refharvrefref nameGrPatentPatentUS5920852 AD Graupe Large memory storage and retrieval LAMSTAR network April 1996ref are fast deep learning neural networks of many layers that can use many filters simultaneously These filters may be nonlinear stochastic logic nonstationary or even nonanalytical They are biologically motivated and learn continuously

A LAMSTAR neural network may serve as a dynamic neural network in spatial or time domains or both Its speed is provided by Hebbian linkweightsref namebook2013aD Graupe Principles of Artificial Neural Networks3rd Edition World Scientific Publishers 2013 pp203274ref that integrate the various and usually different filters preprocessing functions into its many layers and to dynamically rank the significance of the various layers and functions relative to a given learning task This grossly imitates biological learning which integrates various preprocessors cochlea retina etc and cortexes Auditory cortexauditory Visual cortexvisual etc and their various regions Its deep learning capability is further enhanced by using inhibition correlation and its ability to cope with incomplete data or lost neurons or layers even amidst a task It is fully transparent due to its link weights The linkweights allow dynamic determination of innovation and redundancy and facilitate the ranking of layers of filters or of individual neurons relative to a task

LAMSTAR has been applied to many domains including medicalrefCite journallastNigamfirstVivek Prakashlast2Graupefirst2Danieldate20040101titleA neuralnetworkbased detection of epilepsyurlhttpsdxdoiorg101179016164104773026534journalNeurological Researchvolume26issue1pages5560doi101179016164104773026534issn01616412pmid14977058refref name11Cite journallastWaxmanfirstJonathan Alast2Graupefirst2Daniellast3Carleyfirst3David Wdate20100401titleAutomated Prediction of Apnea and Hypopnea Using a LAMSTAR Artificial Neural Networkurlhttpwwwatsjournalsorgdoiabs101164rccm2009071146OCjournalAmerican Journal of Respiratory and Critical Care Medicinevolume181issue7pages727733doi101164rccm2009071146ocissn1073449Xrefref nameGrGrZhcite journallast2Graupefirst2M Hlast3Zhongfirst3Ylast4Jacksonfirst4R Kyear2008titleBlind adaptive filtering for noninvasive extraction of the fetal electrocardiogram and its nonstationaritiesurljournalProc Inst Mech Eng UK Part H Journal of Engineering in Medicinevolume222issue8pages12211234doi10124309544119jeim417last1Graupefirst1Dref and financial predictionsref namebook2013bharvnbGraupe2013pp240253ref adaptive filtering of noisy speech in unknown noiseref nameGrAboncite journallast2Abonfirst2Jyear2002titleA Neural Network for Blind Adaptive Filtering of Unknown Noise from SpeechurlhttpswwwtibeuensearchidBLCPCN019373941BlindAdaptiveFilteringofSpeechfromNoiseofjournalIntelligent Engineering Systems Through Artificial Neural NetworkslanguageenpublisherTechnische Informationsbibliothek TIBvolume12issuepages683688last1Graupefirst1Daccessdate20170614ref stillimage recognitionref namebook2013cD Graupe Principles of Artificial Neural Networks3rd Edition World Scientific Publishers 2013 pp253274ref video image recognitionref nameGiradocite journallast2Sandinfirst2D Jlast3DeFantifirst3T Ayear2003titleRealtime camerabased face detection using a modified LAMSTAR neural network systemurljournalProc SPIE 5015 Applications of Artificial Neural Networks in Image Processing VIIIvolume5015issuepages36pagedoi10111712477405last1Giradofirst1J IseriesApplications of Artificial Neural Networks in Image Processing VIIIbibcode2003SPIE501536Gref software securityref nameVenkSelcite journallast2Selvanfirst2Syear2007titleIntrusion Detection using an Improved Competitive Learning Lamstar NetworkurljournalInternational Journal of Computer Science and Network Securityvolume7issue2pages255263last1Venkatachalamfirst1Vref and adaptive control of nonlinear systemsrefCite weburlhttpswwwresearchgatenetpublication262316982ControlofunstablenonlinearandnonstationarysystemsusingLAMSTARneuralnetworkstitleControl of unstable nonlinear and nonstationary systems using LAMSTAR neural networkslastGraupefirstDlast2Smollackfirst2Mdate2007websiteResearchGatepublisherProceedings of 10th IASTED on Intelligent Control Sect592pages141144languageenarchiveurlarchivedatedeadurlaccessdate20170614ref LAMSTAR had a much faster learning speed and somewhat lower error rate than a CNN based on ReLUfunction filters and max pooling in 20 comparative studiesref namebook1016cite bookurlgoogle books plainurly ide5hIDQAAQBAJpage57titleDeep Learning Neural Networks Design and Case StudieslastGraupefirstDanieldate7 July 2016publisherWorld Scientific Publishing Co Incyearisbn9789813146471locationpages57110ref

These applications demonstrate delving into aspects of the data that are hidden from shallow learning networks and the human senses such as in the cases of predicting onset of sleep apnea eventsref name11  of an electrocardiogram of a fetus as recorded from skinsurface electrodes placed on the mothers abdomen early in pregnancyref nameGrGrZh  of financial predictionref namebook2013  or in blind filtering of noisy speechref nameGrAbon 

LAMSTAR was proposed in 1996 US Patent5920852 A and was further developed Graupe and Kordylewski from 19972002refCite journallastGraupefirstDlast2Kordylewskifirst2HdateAugust 1996titleNetwork based on SOM SelfOrganizingMap modules combined with statistical decision toolsurlhttpieeexploreieeeorgdocument594203journalProceedings of the 39th Midwest Symposium on Circuits and Systemsvolume1pages471474 vol1doi101109mwscas1996594203isbn0780336364refrefCite journallastGraupefirstDlast2Kordylewskifirst2Hdate19980301titleA Large Memory Storage and Retrieval Neural Network for Adaptive Retrieval and Diagnosisurlhttpwwwworldscientificcomdoiabs101142S0218194098000091journalInternational Journal of Software Engineering and Knowledge Engineeringvolume08issue1pages115138doi101142s0218194098000091issn02181940refref nameKordylewcite journallast2Graupefirst2Dlast3Liufirst3Kyear2001titleA novel largememory neural network as an aid in medical diagnosis applicationsurljournalIEEE Transactions on Information Technology in Biomedicinevolume5issue3pages202209doi1011094233945291last1Kordylewskifirst1Href A modified version known as LAMSTAR 2 was developed by Schneider and Graupe in 2008ref nameSchncite journallast2Graupeyear2008titleA modified LAMSTAR neural network and its applicationsurljournalInternational journal of neural systemsvolume18issue4pages331337doi101142s0129065708001634last1Schneiderfirst1NCrefref namebook2013dharvnbGraupe2013p217ref

 Stacked denoising autoencoders 
The auto encoder idea is motivated by the concept of a good representation For example for a Linear classifierclassifier a good representation can be defined as one that yields a betterperforming classifier

An encoder is a deterministic mapping mathfthetamath that transforms an input vector x into hidden representation y where maththeta  boldsymbolW bmath mathboldsymbolWmath is the weight matrix and b is an offset vector bias A decoder maps back the hidden representation y to the reconstructed input z via mathgthetamath The whole process of auto encoding is to compare this reconstructed input to the original and try to minimize the error to make the reconstructed value as close as possible to the original

In stacked denoising auto encoders the partially corrupted output is cleaned denoised This idea was introduced in 2010 by Vincent et alref nameref9cite journallast2Larochellefirst2Hugolast3Lajoiefirst3Isabellelast4Bengiofirst4Yoshualast5Manzagolfirst5PierreAntoinedate2010titleStacked Denoising Autoencoders Learning Useful Representations in a Deep Network with a Local Denoising Criterionurlhttpdlacmorgcitationcfmid1953039journalThe Journal of Machine Learning Researchvolume11pages33713408last1Vincentfirst1Pascalref with a specific approach to good representation a good representation is one that can be obtained Robustness computer sciencerobustly from a corrupted input and that will be useful for recovering the corresponding clean input Implicit in this definition are the following ideas
 The higher level representations are relatively stable and robust to input corruption
 It is necessary to extract features that are useful for representation of the input distribution
The algorithm starts by a stochastic mapping of mathboldsymbolxmath to mathtildeboldsymbolxmath through mathqDtildeboldsymbolxboldsymbolxmath this is the corrupting step Then the corrupted input mathtildeboldsymbolxmath passes through a basic autoencoder process and is mapped to a hidden representation mathboldsymboly  fthetatildeboldsymbolx  sboldsymbolWtildeboldsymbolxbmath From this hidden representation we can reconstruct mathboldsymbolz  gthetaboldsymbolymath In the last stage a minimization algorithm runs in order to have z as close as possible to uncorrupted input mathboldsymbolxmath The reconstruction error mathLHboldsymbolxboldsymbolzmath might be either the crossentropy loss with an affinesigmoid decoder or the squared error loss with an Affine transformationaffine decoderref nameref9 

In order to make a deep architecture auto encoders stackref nameballard1987Cite weburlhttpwwwaaaiorgPapersAAAI1987AAAI87050pdftitleModular learning in neural networkslastBallardfirstDana Hdate1987websiteProceedings of AAAIpages279284archiveurlarchivedatedeadurlaccessdateref Once the encoding function mathfthetamath of the first denoising auto encoder is learned and used to uncorrupt the input corrupted input the second level can be trainedref nameref9 

Once the stacked auto encoder is trained its output can be used as the input to a supervised learning algorithm such as support vector machine classifier or a multiclass logistic regressionref nameref9 

 Deep stacking networks 
A a deep stacking network DSNref nameref17cite journallast2Yufirst2Donglast3Plattfirst3Johndate2012titleScalable stacking and learning for building deep architecturesurlhttpresearchsrvmicrosoftcompubs157586DSNICASSP2012pdfjournal2012 IEEE International Conference on Acoustics Speech and Signal Processing ICASSPpages21332136last1Dengfirst1Liref deep convex network is based on a hierarchy of blocks of simplified neural network modules It was introduced in 2011 by Deng and Dongref nameref16cite journallast2Yufirst2Dongdate2011titleDeep Convex Net A Scalable Architecture for Speech Pattern Classificationurlhttpwwwtruebluenegotiationscomfilesdeepconvexnetworkinterspeech2011pubpdfjournalProceedings of the Interspeechpages22852288last1Dengfirst1Liref It formulates the learning as a convex optimization problem with a Closedform expressionclosedform solution emphasizing the mechanisms similarity to Ensemble learningstacked generalizationref nameref18cite journaldate1992titleStacked generalizationjournalNeural Networksvolume5issue2pages241259doi101016S0893608005800231last1Davidfirst1Wolpertref Each DSN block is a simple module that is easy to train by itself in a Supervised learningsupervised fashion without backpropagation for the entire blocksrefCite journallastBengiofirstYdate20091115titleLearning Deep Architectures for AIurlhttpwwwnowpublisherscomarticleDetailsMAL006journalFoundations and Trends in Machine LearninglanguageEnglishvolume2issue1pages1127doi1015612200000006issn19358237ref

Each block consists of a simplified multilayer perceptron MLP with a single hidden layer The hidden layer h has logistic Sigmoid functionsigmoidal Artificial neuronunits and the output layer has linear units Connections between these layers are represented by weight matrix U inputtohiddenlayer connections have weight matrix W Target vectors t form the columns of matrix T and the input data vectors x form the columns of matrix X The matrix of hidden units is mathboldsymbolH  sigmaboldsymbolWTboldsymbolXmath Modules are trained in order so lowerlayer weights W are known at each stage The function performs the elementwise Logistic functionlogistic sigmoid operation Each block estimates the same final label class y and its estimate is concatenated with original input X to form the expanded input for the next block Thus the input to the first block contains the original data only while downstream blocks input adds the output of preceding blocks Then learning the upperlayer weight matrix U given other weights in the network can be formulated as a convex optimization problem
 mathminUT f  boldsymbolUT boldsymbolH  boldsymbolT2Fmath
which has a closedform solution

Unlike other deep architectures such as DBNs the goal is not to discover the transformed Feature machine learningfeature representation The structure of the hierarchy of this kind of architecture makes parallel learning straightforward as a batchmode optimization problem In purely Discriminative modeldiscriminative tasks DSNs perform better than conventional Deep belief networkDBNnowikisref nameref17 

 Tensor deep stacking networks 
This architecture is a DSN extension It offers two important improvements it uses higherorder information from covariance statistics and it transforms the Convex optimizationnonconvex problem of a lowerlayer to a convex subproblem of an upperlayerref nameref19cite journallast2Dengfirst2Lilast3Yufirst3Dongdate2012titleTensor deep stacking networksjournalIEEE Transactions on Pattern Analysis and Machine Intelligencevolume115issue8pages19441957doi101109tpami2012268last1Hutchinsonfirst1Brianref TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions via a thirdorder tensor

While parallelization and scalability are not considered seriously in conventional HtitleDeep neural networksDNNsref nameref26cite journallast2Salakhutdinovfirst2Ruslandate2006titleReducing the Dimensionality of Data with Neural NetworksjournalSciencevolume313issue5786pages504507doi101126science1127647pmid16873662last1Hintonfirst1Geoffreybibcode2006Sci313504Hrefref nameref27cite journallast2Yufirst2Dlast3Dengfirst3Llast4Acerofirst4Adate2012titleContextDependent PreTrained Deep Neural Networks for LargeVocabulary Speech RecognitionjournalIEEE Transactions on Audio Speech and Language Processingvolume20issue1pages3042doi101109tasl20112134090last1Dahlfirst1Grefref nameref28cite journallast2Dahlfirst2Georgelast3Hintonfirst3Geoffreydate2012titleAcoustic Modeling Using Deep Belief NetworksjournalIEEE Transactions on Audio Speech and Language Processingvolume20issue1pages1422doi101109tasl20112109382last1Mohamedfirst1Abdelrahmanref all learning for HtitleDeep stacking networkDSNs and HtitleTensor deep stacking networkTDSNs is done in batch mode to allow parallelizationref nameref16 ref nameref17  Parallelization allows scaling the design to larger deeper architectures and data sets

The basic architecture is suitable for diverse tasks such as Statistical classificationclassification and Regression analysisregression

 Spikeandslab RBMs 
The need for deep learning with Real numberrealvalued inputs as in Gaussian restricted Boltzmann machines led to the spikeandslab Restricted Boltzmann machineRBM ssRestricted Boltzmann machineRBM which models continuousvalued inputs with strictly Binary variablebinary latent variablesref nameref30cite journallast2Bergstrafirst2Jameslast3Bengiofirst3Yoshuadate2011titleA Spike and Slab Restricted Boltzmann MachineurlhttpmachinelearningwustledumlpaperspaperfilesAISTATS2011CourvilleBB11pdfjournalJMLR Workshop and Conference Proceedingvolume15pages233241last1Courvillefirst1Aaronref Similar to basic Restricted Boltzmann machineRBMs and its variants a spikeandslab Restricted Boltzmann machineRBM is a bipartite graph while like Restricted Boltzmann machineGRBMs the visible units input are realvalued The difference is in the hidden layer where each hidden unit has a binary spike variable and a realvalued slab variable A spike is a discrete probability mass at zero while a slab is a Probability densitydensity over continuous domainref nameref32cite conferencelast1Courvillefirst1Aaronlast2Bergstrafirst2Jameslast3Bengiofirst3YoshuachapterUnsupervised Models of Images by SpikeandSlab RBMstitleProceedings of the 28th International Conference on Machine Learningvolume10pages18date2011urlhttpmachinelearningwustledumlpaperspaperfilesICML2011Courville591pdfref their mixture forms a Prior probabilitypriorref nameref31cite journallast2Beauchampfirst2Jdate1988titleBayesian Variable Selection in Linear RegressionjournalJournal of the American Statistical Associationvolume83issue404pages10231032doi10108001621459198810478694last1Mitchellfirst1Tref

An extension of ssRestricted Boltzmann machineRBM called ¬µssRestricted Boltzmann machineRBM provides extra modeling capacity using additional terms in the energy function One of these terms enables the model to form a Conditional probability distributionconditional distribution of the spike variables by marginalizing out the slab variables given an observation

 Compound hierarchicaldeep models 
Compound hierarchicaldeep models compose deep networks with nonparametric Bayesian networkBayesian models Feature machine learningFeatures can be learned using deep architectures such as DBNsref namehinton20062cite journallast2Osinderofirst2Slast3Tehfirst3Yyear2006titleA fast learning algorithm for deep belief netsurlhttpwwwcstorontoeduhintonabspsfastncpdfjournalNeural Computation journalNeural Computationvolume18issue7pages15271554doi101162neco20061871527pmid16764513last1Hintonfirst1G Eauthorlink1Geoffrey Hintonref DBMsref nameref3cite journallast1Hintonfirst1Geoffreylast2Salakhutdinovfirst2Ruslandate2009titleEfficient Learning of Deep Boltzmann MachinesurlhttpmachinelearningwustledumlpaperspaperfilesAISTATS09SalakhutdinovHpdfvolume3pages448455ref deep auto encodersref nameref15cite journallast2Bengiofirst2Yoshualast3Louradourfirst3Jerdmelast4Lamblinfirst4Pascaldate2009titleExploring Strategies for Training Deep Neural Networksurlhttpdlacmorgcitationcfmid1577070journalThe Journal of Machine Learning Researchvolume10pages140last1Larochellefirst1Hugoref convolutional variantsref nameref39cite journallast2Carpenterfirst2Blakedate2011titleText Detection and Character Recognition in Scene Images with Unsupervised Feature Learningurlhttpwwwiaprtc11orgarchiveicdar2011fileupPDF4520a440pdfjournalvolumepages440445vialast1Coatesfirst1Adamrefref nameref40cite journallast2Grossefirst2Rogerdate2009titleConvolutional deep belief networks for scalable unsupervised learning of hierarchical representationsurlhttpportalacmorgcitationcfmdoid15533741553453journalProceedings of the 26th Annual International Conference on Machine Learningpages18last1Leefirst1Honglakref ssRBMsref nameref32  deep coding networksref nameref41cite journallast2Zhangfirst2Tongdate2010titleDeep Coding NetworkurlhttpmachinelearningwustledumlpaperspaperfilesNIPS20101077pdfjournalAdvances in Neural   pages19last1Linfirst1Yuanqingref DBNs with sparse feature learningref nameref42cite journallast2Boureaufirst2YLandate2007titleSparse Feature Learning for Deep Belief NetworksurlhttpmachinelearningwustledumlpaperspaperfilesNIPS20071118pdfjournalAdvances in Neural Information Processing Systemsvolume23pages18last1Ranzatofirst1Marc Aurelioref RNNsref nameref43cite journallast2Linfirst2Clifdate2011titleParsing Natural Scenes and Natural Language with Recursive Neural NetworksurlhttpmachinelearningwustledumlpaperspaperfilesICML2011Socher125pdfjournalProceedings of the 26th International Conference on Machine Learninglast1Socherfirst1Richardref conditional DBNsref nameref44cite journallast2Hintonfirst2Geoffreydate2006titleModeling Human Motion Using Binary Latent VariablesurlhttpmachinelearningwustledumlpaperspaperfilesNIPS2006693pdfjournalAdvances in Neural Information Processing Systemslast1Taylorfirst1Grahamref denoising auto encodersref nameref45cite journallast2Larochellefirst2Hugodate2008titleExtracting and composing robust features with denoising autoencodersurlhttpportalacmorgcitationcfmdoid13901561390294journalProceedings of the 25th international conference on Machine learning  ICML 08pages10961103last1Vincentfirst1Pascalref This provides a better representation allowing faster learning and more accurate classification with highdimensional data However these architectures are poor at learning novel classes with few examples because all network units are involved in representing the input a visible anchordistributed representation and must be adjusted together high degree of freedom Limiting the degree of freedom reduces the number of parameters to learn facilitating learning of new classes from few examples Hierarchical Bayesian modelHierarchical Bayesian HB models allow learning from few examples for exampleref nameref34cite journallast2Perforsfirst2Amylast3Tenenbaumfirst3Joshuadate2007titleLearning overhypotheses with hierarchical Bayesian modelsjournalDevelopmental Sciencevolume10issue3pages30721doi101111j14677687200700585xpmid17444972last1Kempfirst1Charlesrefref nameref37cite journallast2Tenenbaumfirst2Joshuadate2007titleWord learning as Bayesian inferencejournalPsychol Revvolume114issue2pages24572doi1010370033295X1142245pmid17500627last1Xufirst1Feirefref nameref46cite journallast2Polatkanfirst2Gungordate2011titleThe Hierarchical Beta Process for Convolutional Factor Analysis and Deep LearningurlhttpmachinelearningwustledumlpaperspaperfilesICML2011Chen251pdfjournalMachine Learning   last1Chenfirst1Borefref nameref47cite journallast2Fergusfirst2Robdate2006titleOneshot learning of object categoriesjournalIEEE Transactions on Pattern Analysis and Machine Intelligencevolume28issue4pages594611doi101109TPAMI200679pmid16566508last1FeiFeifirst1Lirefref nameref48cite journallast2Dunsonfirst2Daviddate2008titleThe Nested Dirichlet Processurlhttpamstattandfonlinecomdoifull101198016214508000000553journalJournal of the American Statistical Associationvolume103issue483pages11311154doi101198016214508000000553last1Rodriguezfirst1Abelref for computer vision statistics and cognitive science

Compound HD architectures aim to integrate characteristics of both HB and deep networks The compound HDPDBM architecture is a hierarchical Dirichlet process HDP as a hierarchical model incorporated with DBM architecture It is a full generative model generalized from abstract concepts flowing through the layers of the model which is able to synthesize new examples in novel classes that look reasonably natural All the levels are learned jointly by maximizing a joint Log probabilitylogprobability Score statisticsscoreref nameref38cite journallast2Joshuafirst2Tenenbaumdate2012titleLearning with HierarchicalDeep ModelsjournalIEEE Transactions on Pattern Analysis and Machine Intelligencevolume35issue8pages195871doi101109TPAMI2012269pmid23787346last1Ruslanfirst1Salakhutdinovref

In a DBM with three hidden layers the probability of a visible input mvarnu is
 mathpboldsymbolnu psi  frac1Zsumh esumijWij1nui hj1  sumjlWjl2hj1hl2sumlmWlm3hl2hm3math
where mathboldsymbolh  boldsymbolh1 boldsymbolh2 boldsymbolh3 math is the set of hidden units and mathpsi  boldsymbolW1 boldsymbolW2 boldsymbolW3  math are the model parameters representing visiblehidden and hiddenhidden symmetric interaction terms

A learned DBM model is an undirected model that defines the joint distribution mathPnu h1 h2 h3math One way to express what has been learned is the Discriminative modelconditional model mathPnu h1 h2h3math and a prior term mathPh3math

Here mathPnu h1 h2h3math represents a conditional DBM model which can be viewed as a twolayer DBM but with bias terms given by the states of mathh3math
 mathPnu h1 h2h3  frac1Zpsi h3esumijWij1nui hj1  sumjlWjl2hj1hl2sumlmWlm3hl2hm3math

 Deep predictive coding networks 
A deep predictive coding network DPCN is a Predictive modellingpredictive coding scheme that uses topdown information to empirically adjust the priors needed for a bottomup inference procedure by means of a deep locally connected generative model This works by extracting sparse Feature machine learningfeatures from timevarying observations using a linear dynamical model Then a pooling strategy is used to learn invariant feature representations These units compose to form a deep architecture and are trained by Greedy algorithmgreedy layerwise unsupervised learning The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers

DPCNs predict the representation of the layer by using a topdown approach using the information in upper layer and temporal dependencies from previous statesref nameref56cite arXiveprint13013541first2Joselast2PrincipetitleDeep Predictive Coding Networksdate2013last1Chalasanifirst1RakeshclasscsLGref

DPCNs can be extended to form a Convolutional neural networkconvolutional networkref nameref56 

 Networks with separate memory structures 
Integrating external memory with ANNs dates to early research in distributed representationsref nameHinton Geoffrey E 19842Cite weburlhttprepositorycmueducgiviewcontentcgiarticle2841contextcompscititleDistributed representationslastHintonfirstGeoffrey Edate1984websitearchiveurlarchivedatedeadurlaccessdateref and Teuvo KohonenKohonens selforganizing maps For example in sparse distributed memory or hierarchical temporal memory the patterns encoded by neural networks are used as addresses for contentaddressable memory with neurons essentially serving as address encoders and Binary decoderdecoders However the early controllers of such memories were not differentiable

 LSTMrelated differentiable memory structures 
Apart from long shortterm memory LSTM other approaches also added differentiable memory to recurrent functions For example
 Differentiable push and pop actions for alternative memory networks called neural stack machinesref nameS Das CL Giles p 79S Das CL Giles GZ Sun Learning Context Free Grammars Limitations of a Recurrent Neural Network with an External Stack Memory Proc 14th Annual Conf of the Cog Sci Soc p 79 1992refref nameMozer M C 1993 pp 863870Cite weburlhttpspapersnipsccpaper626aconnectionistsymbolmanipulatorthatdiscoversthestructureofcontextfreelanguagestitleA connectionist symbol manipulator that discovers the structure of contextfree languageslastMozerfirstM Clast2Dasfirst2Sdate1993websitepublisherNIPS 5pages863870archiveurlarchivedatedeadurlaccessdateref
 Memory networks where the control networks external differentiable storage is in the fast weights of another networkref nameReferenceCcite journalyear1992titleLearning to control fastweight memories An alternative to recurrent netsurljournalNeural Computationvolume4issue1pages131139doi101162neco199241131last1Schmidhuberfirst1Jref
 LSTM forget gatesref nameF Gers N Schraudolph 2002cite journallast2Schraudolphfirst2Nlast3Schmidhuberfirst3Jdateyear2002titleLearning precise timing with LSTM recurrent networksurlhttpjmlrorgpapersvolume3gers02agers02apdfjournalJMLRvolume3issuepages115143vialast1Gersfirst1Fref
 Selfreferential RNNs with special output units for addressing and rapidly manipulating the RNNs own weights in differentiable fashion internal storageref nameJ Schmidhuber pages 191195Cite conferenceauthorJ√ºrgen SchmidhubertitleAn introspective network that can learn to run its own weight change algorithmbooktitleIn Proc of the Intl Conf on Artificial Neural Networks Brightonpages191195publisherIEEyear1993urlftpftpidsiachpubjuergeniee93selfpsgzrefref nameHochreiter Sepp 2001cite journallast2Youngerfirst2A Stevenlast3Conwellfirst3Peter Rdateyear2001titleLearning to Learn Using Gradient Descenturlhttpciteseerxistpsueduviewdocsummarydoi10115323journalICANNvolume2130issuepages8794doivialast1Hochreiterfirst1Seppref
 Learning to transduce with unbounded memoryref nameGrefenstette Edward 1506Grefenstette Edward et al httpsarxivorgpdf150602516pdf Learning to Transduce with Unbounded Memoryarxiv150602516 2015ref

 Neural Turing machines 
MainNeural Turing machineNeural Turing machinesref nameGraves Alex 14102Graves Alex Greg Wayne and Ivo Danihelka Neural Turing Machines arxiv14105401 2014ref couple LSTM networks to external memory resources with which they can interact by attentional processes The combined system is analogous to a Turing machine but is differentiable endtoend allowing it to be efficiently trained by gradient descent Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying sorting and associative recall from input and output examples

Differentiable neural computers DNC are an NTM extension They outperformed Neural turing machines long shortterm memory systems and memory networks on sequenceprocessing tasksref name02Cite newsurlhttpswwwwiredcoukarticledeepmindaitubelondonundergroundtitleDeepMinds AI learned to ride the London Underground using humanlike reason and memorylastBurgessfirstMattnewspaperWIRED UKlanguageenGBaccessdate20161019refrefCite newsurlhttpswwwpcmagcomnews348701deepmindailearnstonavigatelondontubetitleDeepMind AI Learns to Navigate London TubenewspaperPCMAGaccessdate20161019refrefCite weburlhttpstechcrunchcom20161013trashed2titleDeepMinds differentiable neural computer helps you navigate the subway with its memorylastMannesfirstJohnwebsiteTechCrunchaccessdate20161019refrefCite journallastGravesfirstAlexlast2Waynefirst2Greglast3Reynoldsfirst3Malcolmlast4Harleyfirst4Timlast5Danihelkafirst5Ivolast6GrabskaBarwi≈Ñskafirst6Agnieszkalast7Colmenarejofirst7Sergio G√≥mezlast8Grefenstettefirst8Edwardlast9Ramalhofirst9Tiagodate20161012titleHybrid computing using a neural network with dynamic external memoryurlhttpwwwnaturecomnaturejournalvaopncurrentfullnature20101htmljournalNaturelanguageenvolume538issue7626doi101038nature20101issn14764687pages471476pmid27732574bibcode2016Natur538471GrefrefCite weburlhttpsdeepmindcomblogdifferentiableneuralcomputerstitleDifferentiable neural computers  DeepMindwebsiteDeepMindaccessdate20161019ref

 Semantic hashing 
Approaches that represent previous experiences directly and Instancebased learninguse a similar experience to form a local model are often called Knearest neighbor algorithmnearest neighbour or Knearest neighbors algorithmknearest neighbors methodsrefcite journallast2Schaalfirst2Stefanyear1995titleMemorybased neural networks for robot learningurljournalNeurocomputingvolume9issue3pages243269doi1010160925231295000336last1Atkesonfirst1Christopher Gref Deep learning is useful in semantic hashingrefSalakhutdinov Ruslan and Geoffrey Hinton httpwwwutstattorontoedursalakhupaperssdarticlepdf Semantic hashing International Journal of Approximate Reasoning 507 2009 969978ref where a deep graphical model the wordcount vectorsref nameLe 2014Cite arXiveprint14054053firstQuoc VlastLefirst2Tomaslast2MikolovtitleDistributed representations of sentences and documentsyear2014classcsCLref obtained from a large set of documentsClarifyreasonverb missingdateJune 2017 Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document Unlike sparse distributed memory that operates on 1000bit addresses semantic hashing works on 32 or 64bit addresses found in a conventional computer architecture

 Memory networks 
Memory networksref nameWeston Jason 14102Weston Jason Sumit Chopra and Antoine Bordes Memory networks arxiv14103916 2014refrefSukhbaatar Sainbayar et al EndToEnd Memory Networks arxiv150308895 2015ref are another extension to neural networks incorporating longterm memory The longterm memory can be read and written to with the goal of using it for prediction These models have been applied in the context of question answering QA where the longterm memory effectively acts as a dynamic knowledge base and the output is a textual responserefBordes Antoine et al Largescale Simple Question Answering with Memory Networks arxiv150602075 2015ref

 Pointer networks 
Deep neural networks can be potentially improved by deepening and parameter reduction while maintaining trainability While training extremely deep eg 1 million layers neural networks might not be practical CPUlike architectures such as pointer networksrefVinyals Oriol Meire Fortunato and Navdeep Jaitly Pointer networks arxiv150603134 2015ref and neural randomaccess machinesrefKurach Karol Andrychowicz Marcin and Sutskever Ilya Neural RandomAccess Machines arxiv151106392 2015ref overcome this limitation by using external randomaccess memory and other components that typically belong to a computer architecture such as Processor registerregisters Arithmetic logic unitALU and Pointer computer programmingpointers Such systems operate on probability distribution vectors stored in memory cells and registers Thus the model is fully differentiable and trains endtoend The key characteristic of these models is that their depth the size of their shortterm memory and the number of parameters can be altered independently  unlike models like LSTM whose number of parameters grows quadratically with memory size

 Encoderdecoder networks 
Encoderdecoder frameworks are based on neural networks that map highly Structured predictionstructured input to highly structured output The approach arose in the context of machine translationrefCite weburlhttpwwwaclweborganthologyD131176titleRecurrent continuous translation modelslastKalchbrennerfirstNlast2Blunsomfirst2Pdate2013websitepublisherEMNLP2013archiveurlarchivedatedeadurlaccessdaterefrefCite weburlhttpspapersnipsccpaper5346sequencetosequencelearningwithneuralnetworkspdftitleSequence to sequence learning with neural networkslastSutskeverfirstIlast2Vinyalsfirst2Odate2014websitepublisherNIPS2014archiveurlarchivedatedeadurlaccessdatelast3Lefirst3Q VrefrefCite journallastChofirstKlast2van Merrienboerfirst2Blast3Gulcehrefirst3Clast4Bougaresfirst4Flast5Schwenkfirst5Hlast6Bengiofirst6YdateOctober 2014titleLearning phrase representations using RNN encoderdecoder for statistical machine translationurlhttpsarxivorgabs14061078journalProceedings of the Empiricial Methods in Natural Language Processingvolume1406pagesarXiv14061078viaarxiv14061078bibcode2014arXiv14061078Cref where the input and output are written sentences in two natural languages In that work an LSTM RNN or CNN was used as an encoder to summarize a source sentence and the summary was decoded using a conditional RNN language model to produce the translationrefCho Kyunghyun Aaron Courville and Yoshua Bengio Describing Multimedia Content using Attentionbased EncoderDecoder Networks arxiv150701053 2015ref These systems share building blocks gated RNNs and CNNs and trained attention mechanisms

 Multilayer kernel machine 
Multilayer kernel machines MKM are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels They use the kernel principal component analysis KPCAref nameref60cite journallast2Smolafirst2Alexanderdate1998titleNonlinear component analysis as a kernel eigenvalue problemjournalNeural computationvolume44issue5pages12991319doi101162089976698300017467last1Scholkopffirst1Bref as a method for the Unsupervised learningunsupervised greedy layerwise pretraining step of deep learningref nameref59cite journaldate2012titleKernel Methods for Deep Learningurlhttpcsewebucsdeduyoc002paperthesisyoungminchopdfpages19last1Chofirst1Youngminref

Layer mathl1math learns the representation of the previous layer mathlmath extracting the mathnlmath Principal component analysisprincipal component PC of the projection layer mathlmath output in the feature domain induced by the kernel For the sake of dimensionality reduction of the updated representation in each layer a Supervised learningsupervised strategy selects the best informative features among features extracted by KPCA The process is
 rank the mathnlmath features according to their mutual information with the class labels
 for different values of K and mathml in1 ldots nlmath compute the classification error rate of a Knearest neighbor KNN classifier using only the mathmlmath most informative features on a validation set
 the value of mathmlmath with which the classifier has reached the lowest error rate determines the number of features to retain
Some drawbacks accompany the KPCA method as the building cells of an MKM

A more straightforward way to use kernel machines for deep learning was developed for spoken language understandingrefCite journallastDengfirstLilast2Turfirst2Gokhanlast3Hefirst3Xiaodonglast4HakkaniT√ºrfirst4Dilekdate20121201titleUse of Kernel Deep Convex Networks and EndToEnd Learning for Spoken Language UnderstandingurlhttpswwwmicrosoftcomenusresearchpublicationuseofkerneldeepconvexnetworksandendtoendlearningforspokenlanguageunderstandingjournalMicrosoft ResearchlanguageenUSref The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units then use Deep learningDeep stacking networksstacking to splice the output of the kernel machine and the raw input in building the next higher level of the kernel machine The number of levels in the deep convex network is a hyperparameter of the overall system to be determined by cross validation

 Neural architecture search 
MainNeural architecture search
Neural architecture search NAS uses machine learning to automate the design of ANNs Various approaches to NAS have designed networks that compare well with handdesigned systems The basic search algorithm is to propose a candidate model evaluate it against a dataset and use the results as feedback to teach the NAS networkrefcite arxivlastZophfirstBarretlast2Lefirst2Quoc Vdate20161104titleNeural Architecture Search with Reinforcement Learningeprint161101578classcsLGref

 Use 

Using ANNs requires an understanding of their characteristics
 Choice of model This depends on the data representation and the application Overly complex models slow learning
 Learning algorithm Numerous tradeoffs exist between learning algorithms Almost any algorithm will work well with the correct hyperparameters for training on a particular data set However selecting and tuning an algorithm for training on unseen data requires significant experimentation
 Robustness If the model cost function and learning algorithm are selected appropriately the resulting ANN can become robust
ANN capabilities fall within the following broad categoriesCitation neededdateJune 2017

 Function approximation or regression analysis including Time seriesPrediction and forecastingtime series prediction fitness approximation and modeling
 Statistical classificationClassification including Pattern recognitionpattern and sequence recognition novelty detection and sequential decision making
 Data processing including filtering clustering blind source separation and compression
 Robotics including directing manipulators and prosthesisprostheses
 Control engineeringControl including computer numerical control

Applications
Because of their ability to reproduce and model nonlinear processes ANNs have found many applications in a wide range of disciplines

Application areas include system identification and control vehicle control trajectory predictionrefcite journallast1Zissisfirst1DimitriostitleA cloud based architecture capable of perceiving and predicting multiple vessel behaviourjournalApplied Soft ComputingdateOctober 2015volume35urlhttpwwwsciencedirectcomsciencearticlepiiS1568494615004329doi101016jasoc201507002pages652661ref process control natural resource management quantum chemistryref nameBalabin2009Cite journaljournalJ Chem Phys volume  131 issue  7 page  074104 doi10106313206326 titleNeural network approach to quantumchemistry data Accurate prediction of density functional theory energies year2009 author1Roman M Balabin author2Ekaterina I Lomakina pmid19708729bibcode  2009JChPh131g4104B ref gameplaying and decision making backgammon chess poker pattern recognition radar systems face identification signal classificationrefcite journallastSenguptafirstNandiniauthor2Sahidullah Mdauthor3Saha GoutamtitleLung sound classification using cepstralbased statistical featuresjournalComputers in Biology and MedicinedateAugust 2016volume75issue1pages118129doi101016jcompbiomed201605013urlhttpwwwsciencedirectcomsciencearticlepiiS0010482516301263ref object recognition and more sequence recognition gesture speech handwritten and printed text recognition medical diagnosis financerefcite journallast1Frenchfirst1JordantitleThe time travellers CAPMjournalInvestment Analysts Journalvolume46issue2pages8196doi1010801029352320161255469urlhttpwwwtandfonlinecomdoiabs1010801029352320161255469year2016ref eg algorithmic tradingautomated trading systems data mining visualization machine translation social network filteringrefCite newsurlhttpswwwwsjcomarticlesfacebookboostsaitoblockterroristpropaganda1497546000titleFacebook Boosts AI to Block Terrorist PropagandalastSchechnerfirstSamdate20170615workWall Street Journalaccessdate20170616languageenUSissn00999660ref and email spam filtering

ANNs have been used to diagnose cancers including lung cancerrefcite weblastGanesanfirstNtitleApplication of Neural Networks in Diagnosing Cancer Disease Using Demographic Dataurlhttpwwwijcaonlineorgjournalnumber26pxc387783pdfpublisherInternational Journal of Computer Applicationsref prostate cancer colorectal cancerrefcite weburlhttpwwwlccumaesjjarecidiva042pdftitleArtificial Neural Networks Applied to Outcome Prediction for Colorectal Cancer Patients in Separate InstitutionslastBottacifirstLeonardopublisherThe Lancetref and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape informationrefcite journallast2Lyonsfirst2Samanthe Mlast3Castlefirst3Jordan Mlast4Prasadfirst4Ashokdate2016titleMeasuring systematic changes in invasive cancer cell shape using Zernike momentsurlhttppubsrscorgenContentArticleLanding2016IBC6IB00100AdivAbstractjournalIntegrative Biologyvolume8issue11pages11831193doi101039C6IB00100Apmid27735002last1Alizadehfirst1Elahehrefrefcite journaldate2016titleChanges in cell shape are correlated with metastatic potential in murineurlhttpbiobiologistsorgcontent53289journalBiology Openvolume5issue3pages289299doi101242bio013409last1Lyonsfirst1Samantheref

ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disastersrefcite arxivlastNabianfirstMohammad Aminlast2Meidanifirst2Hadidate20170828titleDeep Learning for Accelerated Reliability Analysis of Infrastructure Networkseprint170808551classcsCErefrefCite journallastNabianfirstMohammad Aminlast2Meidanifirst2Hadidate2018titleAccelerating Stochastic Assessment of PostEarthquake Transportation Network Connectivity via MachineLearningBased Surrogatesurlhttpstridtrborgview1496617journalTransportation Research Board 97th Annual Meetingvolumepagesviaref

ANNs have also been used for building blackbox models in geoscience hydrologyrefCite journallastnull nulldate20000401titleArtificial Neural Networks in Hydrology I Preliminary Conceptsurlhttpascelibraryorgdoiabs101061ASCE10840699200052115journalJournal of Hydrologic Engineeringvolume5issue2pages115123doi101061ASCE10840699200052115refrefCite journallastnull nulldate20000401titleArtificial Neural Networks in Hydrology II Hydrologic Applicationsurlhttpascelibraryorgdoiabs101061ASCE10840699200052124journalJournal of Hydrologic Engineeringvolume5issue2pages124137doi101061ASCE10840699200052124ref ocean modelling and coastal engineeringrefCite journallastPeresfirstD Jlast2Iuppafirst2Clast3Cavallarofirst3Llast4Cancellierefirst4Alast5Fotifirst5Edate20151001titleSignificant wave height record extension by neural networks and reanalysis wind dataurlhttpwwwsciencedirectcomsciencearticlepiiS1463500315001432journalOcean Modellingvolume94pages128140doi101016jocemod201508002bibcode2015OcMod94128PrefrefCite journallastDwarakishfirstG Slast2Rakshithfirst2Shettylast3Natesanfirst3Ushadate2013titleReview on Applications of Neural Network in Coastal EngineeringurlhttpwwwciitresearchorgdlindexphpaimlarticleviewAIML072013007journalArtificial Intelligent Systems and Machine LearninglanguageEnglishvolume5issue7pages324331ref and geomorphologyrefCite journallastErminifirstLeonardolast2Catanifirst2Filippolast3Casaglifirst3Nicoladate20050301titleArtificial Neural Networks applied to landslide susceptibility assessmenturlhttpwwwsciencedirectcomsciencearticlepiiS0169555X04002272journalGeomorphologyseriesGeomorphological hazard and human impact in mountain environmentsvolume66issue1pages327343doi101016jgeomorph200409025bibcode2005Geomo66327Eref are just few examples of this kind

Neuroscience
Theoretical neuroscience is concerned with the theoretical analysis and the computational modeling of biological neural systems

To gain this understanding neuroscientists are interested in linking observed biological processes to biologically plausible mechanisms for neural processing and learning biological neural network models

Brain research has repeatedly led to new ANN approaches such as the use of connections to connect neurons in other layers rather than adjacent neurons in the same layer Other research explored the use of multiple signal types or finer control than Boolean algebraboolean onoff variables Dynamic neural networks can dynamically form new connections and even new neural units while disabling othersrefCite weburlhttpswwwmathworkscomhelpnnetugintroductiontodynamicneuralnetworkshtmltitleIntroduction to Dynamic Neural Networks  MATLAB  Simulinkwebsitewwwmathworkscomaccessdate20170615ref

Types of models
Many types of models are used defined at different levels of abstraction and modeling different aspects of neural systems They range from models of the shortterm behavior of biological neuron modelsindividual neuronsrefcite journal  authorForrest MD titleSimulation of alcohol action upon a detailed Purkinje neuron model and a simpler surrogate model that runs 400 times faster journal BMC Neuroscience  volume16 issue27  dateApril 2015 doi101186s1286801501626 urlhttpwwwbiomedcentralcom147122021627 ref models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems These include models of the longterm and shortterm plasticity of neural systems and their relations to learning and memory from the individual neuron to the system level

Theoretical properties
Computational power

The multilayer perceptron is a universal function approximator as proven by the universal approximation theorem However the proof is not constructive regarding the number of neurons required the network topology the weights and the learning parameters

A specific recurrent architecture with rational valued weights as opposed to full precision real numbervalued weights has the full power of a Universal Turing Machineuniversal Turing machinerefCite journal title   Turing computability with neural nets  url  httpwwwmathrutgersedusontagFTPDIRamlturingpdf  year  1991  journal  Appl Math Lett  pages  7780  volume  4  issue  6  last1  Siegelmann  first1   HT  last2   Sontag  first2   ED  doi   101016089396599190080F ref using a finite number of neurons and standard linear connections Further the use of irrational values for weights results in a machine with HypercomputationsuperTuring powerrefcite journal last1Balc√°zar first1Jos√© titleComputational Power of Neural Networks A Kolmogorov Complexity Characterization journalInformation Theory IEEE Transactions on dateJul 1997 volume43 issue4 pages11751183 doi10110918605580 urlhttpieeexploreieeeorgxplloginjsptparnumber605580urlhttp3A2F2Fieeexploreieeeorg2Fxpls2Fabsalljsp3Farnumber3D605580 accessdate3 November 2014citeseerx10114117782 ref

Capacity
Models capacity property roughly corresponds to their ability to model any given function It is related to the amount of information that can be stored in the network and to the notion of complexitycitation neededdateFebruary 2017

Convergence
Models may not consistently converge on a single solution firstly because many local minima may exist depending on the cost function and the model Secondly the optimization method used might not guarantee to converge when it begins far from any local minimum Thirdly for sufficiently large data or parameters some methods become impractical However for cerebellar model articulation controllerCMAC neural network a recursive least squares algorithm was introduced to train it and  this algorithm can be guaranteed to converge in one stepref nameQin1

Generalization and statistics
Applications whose goal is to create a system that generalizes well to unseen examples face the possibility of overtraining This arises in convoluted or overspecified systems when the capacity of the network significantly exceeds the needed free parameters Two approaches address overtraining The first is to use crossvalidation statisticscrossvalidation and similar techniques to check for the presence of overtraining and optimally select hyperparameters to minimize the generalization error The second is to use some form of regularization mathematicsregularization This concept emerges in a probabilistic Bayesian framework where regularization can be performed by selecting a larger prior probability over simpler models but also in statistical learning theory where the goal is to minimize over two quantities the empirical risk and the structural risk which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting

FileSynapse deploymentjpgthumbrightConfidence analysis of a neural network
Supervised neural networks that use a mean squared error MSE cost function can use formal statistical methods to determine the confidence of the trained model The MSE on a validation set can be used as an estimate for variance This value can then be used to calculate the confidence interval of the output of the network assuming a normal distribution A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified

By assigning a softmax activation function a generalization of the logistic function on the output layer of the neural network or a softmax component in a componentbased neural network for categorical target variables the outputs can be interpreted as posterior probabilities This is very useful in classification as it gives a certainty measure on classifications

The softmax activation function is

mathyifracexisumj1c exjmath
section endtheory 

Criticism

Training issues
A common criticism of neural networks particularly in robotics is that they require too much training for realworld operationCitation neededdateNovember 2014 Potential solutions include randomly shuffling training examples by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example and by grouping examples in socalled minibatches Improving the training efficiency and convergence capability has always been an ongoing research area for neural network For example by introducing a recursive least squares algorithm for cerebellar model articulation controllerCMAC neural network the training process only takes one step to convergeref nameQin1

Theoretical issues

No neural network has solved computationally difficult problems such as the Eight queens puzzlenQueens problem the travelling salesman problem or the problem of Integer factorizationfactoring large integers

A fundamental objection is that they do not reflect how real neurons function Back propagation is a critical part of most artificial neural networks although no such mechanism exists in biological neural networksrefcite journal  last1  Crick  first1  Francis  year  1989  title  The recent excitement about neural networks  journal  Nature  volume  337  issue  6203  pages  129132  doi  101038337129a0  url  httpeuropepmcorgabstractmed2911347  pmid2911347 bibcode  1989Natur337129C ref How information is coded by real neurons is not known Sensory neuronSensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequentlyrefcite journal  last1  Adrian  first1  Edward D  year  1926  title  The impulses produced by sensory nerve endings  journal  The Journal of Physiology  volume  61  issue  1  pages  4972  doi  101113jphysiol1926sp002273  pmid  16993776  pmc  1514809  url  httponlinelibrarywileycomdoi101113jphysiol1926sp002273full ref Other than the case of relaying information from a sensor neuron to a motor neuron almost nothing of the principles of how information is handled by biological neural networks is known

The motivation behind ANNs is not necessarily to strictly replicate neural function but to use biological neural networks as an inspiration A central claim of ANNs is therefore that it embodies some new and powerful general principle for processing information Unfortunately these general principles are illdefined It is often claimed that they are Emergent propertiesemergent from the network itself This allows simple statistical association the basic function of artificial neural networks to be described as learning or recognition Alexander Dewdney commented that as a result artificial neural networks have a somethingfornothing quality one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are No human hand or mind intervenes solutions are found as if by magic and no one it seems has learned anythingrefcite bookurlgoogle books plainurly idKcHaAAAAMAAJpage82titleYes we have no neutrons an eyeopening tour through the twists and turns of bad sciencelastDewdneyfirstA Kdate1 April 1997publisherWileyyearisbn9780471108061locationpages82ref

Biological brains use both shallow and deep circuits as reported by brain anatomyref nameVanEssen1991D J Felleman and D C Van Essen httpcercoroxfordjournalsorgcontent1111fullpdfhtml Distributed hierarchical processing in the primate cerebral cortex Cerebral Cortex 1 pp 147 1991ref displaying a wide variety of invariance Wengref nameWeng2012J Weng httpswwwamazoncomNaturalArtificialIntelligenceIntroductionComputationaldp0985875720 Natural and Artificial Intelligence Introduction to Computational BrainMind BMI Press ISBN9780985875725 2012ref argued that the brain selfwires largely according to signal statistics and therefore a serial cascade cannot catch all major statistical dependencies

Hardware issues
Large and effective neural networks require considerable computing resourcesref name0cite journallast1Edwardsfirst1ChristitleGrowing pains for deep learningjournalCommunications of the ACMdate25 June 2015volume58issue7pages1416doi1011452771283ref While the brain has hardware tailored to the task of processing signals through a Graph discrete mathematicsgraph of neurons simulating even a simplified neuron on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connectionssnd which can consume vast amounts of Randomaccess memorymemory and storage Furthermore the designer often needs to transmit signals through many of these connections and their associated neuronssnd which must often be matched with enormous Central processing unitCPU processing power and time

J√ºrgen SchmidhuberSchmidhuber notes that the resurgence of neural networks in the twentyfirst century is largely attributable to advances in hardware from 1991 to 2015 computing power especially as delivered by Generalpurpose computing on graphics processing unitsGPGPUs on Graphics processing unitGPUs has increased around a millionfold making the standard backpropagation algorithm feasible for training networks that are several layers deeper than beforerefcite journal lastSchmidhuber firstJ√ºrgen titleDeep learning in neural networks An overview journalNeural Networks volume61 year2015 pages85117 arxiv14047828 doi101016jneunet201409003pmid25462637 ref The use of parallel GPUs can reduce training times from months to daysr0

Neuromorphic engineering addresses the hardware difficulty directly by constructing nonvonNeumann chips to directly implement neural networks in circuitry Another chip optimized for neural network processing is called a Tensor Processing Unit or TPUrefcite news urlhttpswwwwiredcom201605googletpucustomchips authorCade Metz newspaperWired dateMay 18 2016 titleGoogle Built Its Very Own Chips to Power Its AI Botsref

Practical counterexamples to criticisms
Arguments against Dewdneys position are that neural networks have been successfully used to solve many complex and diverse tasks ranging from autonomously flying aircraftrefhttpwwwnasagovcentersdrydennewsNewsReleases20030349html NASA  Dryden Flight Research Center  News Room News Releases NASA NEURAL NETWORK PROJECT PASSES MILESTONE Nasagov Retrieved on 20131120ref to detecting credit card fraud to mastering the game of Go gameGo

Technology writer Roger Bridgman commented

blockquoteNeural networks for instance are in the dock not only because they have been hyped to high heaven what hasnt but also because you could create a successful net without understanding how it worked the bunch of numbers that captures its behaviour would in all probability be an opaque unreadable tablevalueless as a scientific resource
In spite of his emphatic declaration that science is not technology Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers An unreadable table that a useful machine could read would still be well worth havingrefhttpmembersfortunecitycomtemplarseriespopperhtml Roger Bridgmans defence of neural networksref
blockquote

Although it is true that analyzing what has been learned by an artificial neural network is difficult it is much easier to do so than to analyze what has been learned by a biological neural network Furthermore researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful For example local vs nonlocal learning and shallow vs deep architecturerefcite weburlhttpwwwiroumontrealcalisapublications2indexphppublicationsshow4titleScaling Learning Algorithms towards AI  LISA  Publications  Aigaion 20publisherref

Hybrid approaches
Advocates of hybrid models combining neural networks and symbolic approaches claim that such a mixture can better capture the mechanisms of the human mindrefSun and Bookman 1990refrefCite journal last1  Tahmasebi  last2  Hezarkhani  title   A hybrid neural networksfuzzy logicgenetic algorithm for grade estimation  url  httpwwwsciencedirectcomsciencearticlepiiS0098300412000398  year  2012 journal  Computers  Geosciences  pages  1827  volume  42 doi   101016jcageo201202004  bibcode  2012CG4218T ref

Types
 Split to Types of artificial neural networks 
MainTypes of artificial neural networks

Artificial neural networks have many variations The simplest static types have one or more static components including number of units number of layers unit weights and topology Dynamic types allow one or more of these to change during the learning process The latter are much more complicated but can shorten learning periods and produce better results Some types allowrequire learning to be supervised by the operator while others operate independently Some types operate purely in hardware while others are purely software and run on general purpose computers

Gallery
gallery widths260
FileSingle layer annsvgA singlelayer feedforward artificial neural network Arrows originating from mathscriptstyle x2math are omitted for clarity  There are p inputs to this network and q outputs In this system the value of the qth output mathscriptstyle yqmath would be calculated as mathscriptstyle yq  Ksumxiwiqbq math
FileTwo layer annsvgA twolayer feedforward artificial neural network
FileArtificial neural networksvgAn artificial neural network
FileAnn dependency graphsvgAn ANN dependency graph
FileSinglelayer feedforward artificial neural networkpngA singlelayer feedforward artificial neural network with 4 inputs 6 hidden and 2 outputs Given position state and direction outputs wheel based control values
FileTwolayer feedforward artificial neural networkpngA twolayer feedforward artificial neural network with 8 inputs 2x8 hidden and 2 outputs Given position state direction and other environment values outputs thruster based control values
FilecmacjpgParallel pipeline structure of CMAC neural network This learning algorithm can converge in one step 
gallery

 See also 
too many see alsosdateMarch 2018
columnslistcolwidth25em
 Hierarchical Deep Learning
 Hierarchical temporal memory
 20Q
 ADALINE
 Adaptive resonance theory
 Artificial life
 Associative Memory BaseAssociative memory
 Autoencoder
 BEAM robotics
 Biological cybernetics
 Biologically inspired computing
 Blue Brain Project
 Catastrophic interference
 Cerebellar Model Articulation Controller CMAC
 Cognitive architecture
 Cognitive science
 Convolutional neural network CNN
 Connectionist expert system
 Connectomics
 Cultured neuronal networks
 Deep learning
 Digital morphogenesis
 Encog
 Fuzzy logic
 Gene expression programming
 Genetic algorithm
 Genetic programming
 Group method of data handling
 Habituation
 In Situ Adaptive Tabulation
 List of machine learning conceptsMachine learning concepts
 Models of neural computation
 Neuroevolution
 Neural coding
 Neural gas
 Neural machine translation
 Neural network software
 Neuroscience
 Ni1000 chip
 Nonlinear system identification
 Optical neural network
 Parallel Constraint Satisfaction Processes
 Parallel distributed processing
 Radial basis function network
 Recurrent neural networks
 Selforganizing map
 Spiking neural network 
 Systolic array
 Tensor product network
 Time delay neural network TDNN


References
Reflist30em

Bibliography
 Cite journal authorBhadeshia H K D H  year1999 titleNeural Networks in Materials Science  journalISIJ International  volume39 pages966979  doi102355isijinternational39966  urlhttpwwwmsmcamacukphasetransabstractsneuralreviewpdf issue10
 Cite bookurlhttpswwwworldcatorgoclc33101074titleNeural networks for pattern recognitionlastMfirstBishop Christopherdate1995publisherClarendon Pressisbn0198538499oclc33101074 
 cite booktitleMathematics of Control Signals and SystemslastCybenkofirstGVpublisherSpringer Internationalyear2006editorlastvan SchuppeneditorfirstJan HchapterApproximation by Superpositions of a Sigmoidal functionchapterurlgoogle books plainurly id4RtVAAAAMAAJpage303pp303314 httpactcommdartmouthedugvcpapersapproxbysuperpositionpdf PDF
 Cite bookurlhttpswwwworldcatorgoclc35558945titleYes we have no neutrons  an eyeopening tour through the twists and turns of bad sciencelastDewdney firstA Kisbn9780471108061oclc35558945year1997publisherWileylocationNew York
 Cite bookurlhttpswwwworldcatorgoclc41347061titlePattern classificationfirstRichard OlastDudalast2Hart first2Peter Elliotlast3Stork first3David Gyear2001publisherWileyisbn0471056693oclc41347061edition2
 Cite journal  last1EgmontPetersenfirst1M last2de Ridder first2D last3Handels first3H  year2002  titleImage processing with neural networks  a review  journalPattern Recognition  volume35  pages22792301  doi  101016S0031320301001789  issue10
 Cite bookurlhttpswwwworldcatorgoclc37875698titleAn introduction to neural networkslastGurney firstKevin year1997publisherUCL Pressisbn1857286731oclc37875698
 Cite bookurlhttpswwwworldcatorgoclc38908586titleNeural networks  a comprehensive foundationlastHaykinfirst Simon Syear1999publisherPrentice Hallisbn0132733501oclc38908586
 cite weblast1Fahlman first1S last2Lebiere first2C year1991 titleThe CascadeCorrelation Learning Architectureurlhttpwwwcsiastateeduhonavarfahlmanpdfcreated for National Science Foundation Contract Number EET8716324  and Defense Advanced Research Projects Agency DOD ARPA Order No 4976 under Contract F3361587C1499 
 Cite bookurlhttpswwwworldcatorgoclc21522159titleIntroduction to the theory of neural computationlast1Hertz first1Jlast3Kroghfirst3Anders Sfirst2Richard Glast2Palmeryear1991publisherAddisonWesley isbn0201515601oclc21522159
 Cite bookurlhttpswwwworldcatorgoclc32179420titleIntroduction to neural networks  design theory and applicationslastLawrencefirstJeanetteyear1994publisherCalifornia Scientific Softwareisbn1883157005oclc32179420
 Cite booktitleInformation theory inference and learning algorithmspublisherCambridge University Pressisbn9780521642989oclc52377690
 cite book lastMacKay  firstDavid JC authorlinkDavid JC MacKayyear2003publisherCambridge University Press isbn9780521642989urlhttpwwwinferencephycamacukitprnnbookpdftitleInformation Theory Inference and Learning Algorithmsrefharv
 Cite bookurlhttpswwwworldcatorgoclc29877717titleSignal and image processing with neural networks  a C sourcebookfirstTimothylastMastersyear1994publisherJ Wileyisbn0471049638oclc29877717
 cite bookurlgoogle books plainurly idm12UR8QmLqoCtitlePattern Recognition and Neural NetworkslastRipleyfirstBrian DauthorlinkBrian D RipleypublisherCambridge University Pressyear2007isbn9780521717700
 cite journallast1Siegelmann first1HT first2Eduardo Dlast2Sontagyear1994titleAnalog computation via neural networks journalTheoretical Computer Science volume 131 issue 2 pp331360urlhttpspdfssemanticscholarorg861ede32115d157e1568622b153e7ed3dca28467pdf doi1010160304397594901783 
 Cite bookurlhttpswwwworldcatorgoclc27145760titleNeural networks for statistical modelinglast1Smith first1Murraydate1993publisherVan Nostrand Reinholdisbn0442013108oclc27145760
 Cite bookurlhttpswwwworldcatorgoclc27429729titleAdvanced methods in neural computinglastWasserman firstPhilip Dyear1993publisherVan Nostrand Reinholdisbn0442004613oclc27429729
 Cite bookurlhttpswwwworldcatorgoclc837524179titleComputational intelligence  a methodological introductionfirst1RudolflastKrusefirst2Christianlast2Borgeltfirst3Flast3Klawonnfirst4Christianlast4Moewesfirst5Matthiaslast5Steinbrecherfirst6Pascallast6Heldyear2013publisherSpringerisbn9781447150121oclc837524179
 Cite bookurlhttpswwwworldcatorgoclc76538146titleNeuroFuzzySysteme  von den Grundlagen k√ºnstlicher Neuronaler Netze zur Kopplung mit FuzzySystemenfirstChristianlastBorgeltyear2003publisherViewegisbn9783528252656oclc76538146

External links

Do not post software links here If you must use the Neural Network Software article Remember however that Wikipedia is neither a link repository nor the place to promote products or show off your code

wikibooksArtificial Neural Networks
 dmozComputersArtificialIntelligenceNeuralNetworksNeural Networks
 httpwwwdkrieselcomenscienceneuralnetworks A brief introduction to Neural Networks PDF illustrated 250p textbook covering the common kinds of neural networks CC license
 httpdeeplearning4jorgneuralnetoverviewhtml An Introduction to Deep Neural Networks
 httppeoplerevoleducomkarditutorialNeuralNetworkindexhtml A Tutorial of Neural Network in Excel
 youtubeidq0pm3BrIUFo titleMIT course on Neural Networks 
 httpswwwacademiaedu25708860AConciseIntroductiontoMachineLearningwithArtificialNeuralNetworks A Concise Introduction to Machine Learning with Artificial Neural Networks
 httpswwwcourseraorgcourseneuralnets Neural Networks for Machine Learning  a course by Geoffrey Hinton
 httpwwwdeeplearningbookorg Deep Learning
 httpethesesuinmalangacid3783 Aplikasi pendeteksi fraud pada event log proses bisnis pengadaan barang dan jasa menggunakan algoritma heuristic miner

CPU technologies

DEFAULTSORTArtificial Neural Network
CategoryComputational statistics
CategoryArtificial neural networks 
CategoryClassification algorithms
CategoryComputational neuroscience
CategoryMarket research
CategoryMarket segmentation
CategoryMathematical psychology
CategoryMathematical and quantitative methods economics

roRe»õea neuralƒÉ artificialƒÉ
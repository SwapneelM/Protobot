parent_config: './conf/jack.yaml'

# loader to use in experiment
loader: 'jack'

# How large the support should be. Can be used for cutting or filtering QA examples
max_support_length: 600

train: 'data/triviaqa/web-train.json'
dev: 'data/triviaqa/web-dev.json'
test: null

# cache preprocessed examples on file in JACK_TEMP to avoid RAM problems
file_cache: True

# [word2vec], [glove] or [memory_map_dir] format of embeddings to be loaded
embedding_format: 'memory_map_dir'

# embeddings to be loaded
embedding_file: 'data/GloVe/glove.840B.300d.memory_map_dir'

# Use fixed vocab of pretrained embeddings
vocab_from_embeddings: True

epochs: 3

dropout: 0.2

batch_size: 16

lowercase: False

# default take all, if set to >0 will be used to select only the top supports based on tf idf with question
max_num_support: 6
# set to -1 if you want to use all during training (which will of course demand more training time)
# paragraphs are subsampled from the top `max_num_support`, the best paragraph is sampled twice as likely as rest
max_training_support: 2

max_span_size: 8

learning_rate: 0.001
min_learning_rate: 0.0001
learning_rate_decay: 0.5
validation_interval: 2000
num_dev_examples=1000

# 'sum' (loss for summed prob. over all possible gold answer spans), 'max' (loss for best span)
loss: 'sum'
